{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44abeade-5fe3-40d6-99cb-621c66badf2e",
   "metadata": {},
   "source": [
    "# Preprocess Combined Data\n",
    "Using the combined QBJ as input, perform data preprocessing including:\n",
    "\n",
    "1. Expand Contractions, Tokenize, and Convert to Lowercase\n",
    "2. Remove Punctuation\n",
    "3. Remove Stop Words\n",
    "4. Remove Words Starting with a Digit\n",
    "5. Parts of Speech (POS) Tagging\n",
    "6. Lemmatize\n",
    "7. Stemming\n",
    "8. Create Bag of Words (BOW)\n",
    "9. Calculate Term Frequency\n",
    "10. Calculate Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "11. Sentencize\n",
    "     1. Lemmatize Sentences\n",
    "     1. Stem Sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e783f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b12a20c0-f0d4-47ba-bc75-32ef8cc62c14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Readthe combined data into a dataframe\n",
    "data_file = \"./data/stress_urinary_incontinence.csv\"\n",
    "\n",
    "# Read the data into a pandas dataframe\n",
    "df = pd.read_csv(\n",
    "    data_file,  # The data file being read, from the variable assignment above\n",
    "    on_bad_lines=\"warn\",  # This tells Pandas to only warn on bad lines vs causing an error\n",
    "    dtype=\"str\",\n",
    ")  # This tells Pandas to treat all numbers as words\n",
    "\n",
    "df.fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6021374b-7c6c-48c7-9319-47b7512b9a0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9605, 37)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85fd9d2d-01a6-45e6-8807-9e27dc01d5d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>MDR_REPORT_KEY</th>\n",
       "      <th>MDR_TEXT_KEY</th>\n",
       "      <th>TEXT_TYPE_CODE</th>\n",
       "      <th>PATIENT_SEQUENCE_NUMBER</th>\n",
       "      <th>DATE_REPORT</th>\n",
       "      <th>FOI_TEXT</th>\n",
       "      <th>DEVICE_EVENT_KEY</th>\n",
       "      <th>IMPLANT_FLAG</th>\n",
       "      <th>DATE_REMOVED_FLAG</th>\n",
       "      <th>...</th>\n",
       "      <th>LOT_NUMBER</th>\n",
       "      <th>OTHER_ID_NUMBER</th>\n",
       "      <th>DEVICE_AVAILABILITY</th>\n",
       "      <th>DATE_RETURNED_TO_MANUFACTURER</th>\n",
       "      <th>DEVICE_REPORT_PRODUCT_CODE</th>\n",
       "      <th>DEVICE_AGE_TEXT</th>\n",
       "      <th>DEVICE_EVALUATED_BY_MANUFACTUR</th>\n",
       "      <th>COMBINATION_PRODUCT_FLAG</th>\n",
       "      <th>UDI-DI</th>\n",
       "      <th>UDI-PUBLIC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>106741</td>\n",
       "      <td>6383024</td>\n",
       "      <td>106903842</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>BASED ON ADDITIONAL INFORMATION RECEIVED THIS ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>OTN</td>\n",
       "      <td>DA</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106742</td>\n",
       "      <td>6383024</td>\n",
       "      <td>106903843</td>\n",
       "      <td>D</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>BASED ON ADDITIONAL INFORMATION RECEIVED THIS ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>OTN</td>\n",
       "      <td>DA</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0 MDR_REPORT_KEY MDR_TEXT_KEY TEXT_TYPE_CODE  \\\n",
       "0     106741        6383024    106903842              N   \n",
       "1     106742        6383024    106903843              D   \n",
       "\n",
       "  PATIENT_SEQUENCE_NUMBER DATE_REPORT  \\\n",
       "0                       1               \n",
       "1                       1               \n",
       "\n",
       "                                            FOI_TEXT DEVICE_EVENT_KEY  \\\n",
       "0  BASED ON ADDITIONAL INFORMATION RECEIVED THIS ...                    \n",
       "1  BASED ON ADDITIONAL INFORMATION RECEIVED THIS ...                    \n",
       "\n",
       "  IMPLANT_FLAG DATE_REMOVED_FLAG  ... LOT_NUMBER OTHER_ID_NUMBER  \\\n",
       "0                                 ...                              \n",
       "1                                 ...                              \n",
       "\n",
       "  DEVICE_AVAILABILITY DATE_RETURNED_TO_MANUFACTURER  \\\n",
       "0                   N                                 \n",
       "1                   N                                 \n",
       "\n",
       "  DEVICE_REPORT_PRODUCT_CODE DEVICE_AGE_TEXT DEVICE_EVALUATED_BY_MANUFACTUR  \\\n",
       "0                        OTN              DA                              N   \n",
       "1                        OTN              DA                              N   \n",
       "\n",
       "  COMBINATION_PRODUCT_FLAG UDI-DI UDI-PUBLIC  \n",
       "0                        N                    \n",
       "1                        N                    \n",
       "\n",
       "[2 rows x 37 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fd1319-1714-4115-9bb3-07935554919f",
   "metadata": {},
   "source": [
    "## Assign a Row ID for Verification\n",
    "Assign a value to a variable that identifies a row from the dataset.  \n",
    "\n",
    "This will allow the same row to be used for verification of each preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2746674c-c24b-4986-b047-54f45d1beeab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "verification_row = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cccf441-a429-4612-bda4-eafbc2af9ef0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load the Natural Language Toolkit (NLTK) and Preprocessing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cbed954-f2f1-41da-a1e8-3718d6ce413f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the NLTK library\n",
    "import nltk  # If this step fails, rerun 07-Install-NLTK.ipynb\n",
    "import string\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b62b98e-4f7e-4025-81e7-0e04f25dd328",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Expand Contractions, Tokenize, and Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b2e40a5-3775-4ff2-94db-6df8f183131c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [based, on, additional, information, received,...\n",
       "1    [based, on, additional, information, received,...\n",
       "2    [if, information, is, provided, in, the, futur...\n",
       "3    [manufacturer, reference, number:, (b)(4)., in...\n",
       "4    [the, patient's, attorney, alleged, a, deficie...\n",
       "Name: TOKENIZED_TEXT, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This approach takes the FOI_TEXT as a string and creates a new column with tokens\n",
    "# It removes contractions _and_ tokenizes at the same time\n",
    "# No additional function is needed, x.split tokenizes the string (FOI text) at every space\n",
    "# A call to lower() converts the word to lowercase\n",
    "\n",
    "df[\"TOKENIZED_TEXT\"] = df[\"FOI_TEXT\"].apply(\n",
    "    lambda x: [contractions.fix(word).lower() for word in x.split()]\n",
    ")\n",
    "df[\"TOKENIZED_TEXT\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49be4fc3-c02f-468b-9dc8-82506eaaeb12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['manufacturer',\n",
       " 'reference',\n",
       " 'number:',\n",
       " '(b)(4).',\n",
       " 'incident',\n",
       " 'date',\n",
       " 'was',\n",
       " 'not',\n",
       " 'provided.',\n",
       " 'lot',\n",
       " 'number',\n",
       " 'not',\n",
       " 'provided.',\n",
       " 'udi',\n",
       " 'not',\n",
       " 'provided',\n",
       " '.',\n",
       " 're-processing',\n",
       " 'information',\n",
       " 'not',\n",
       " 'provided.',\n",
       " 'since',\n",
       " 'the',\n",
       " 'lot',\n",
       " 'number',\n",
       " 'was',\n",
       " 'not',\n",
       " 'provided,',\n",
       " 'this',\n",
       " 'information',\n",
       " 'cannot',\n",
       " 'be',\n",
       " 'determined.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"TOKENIZED_TEXT\"][verification_row]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b341a0-4cc5-409b-a4f9-510cff03c266",
   "metadata": {},
   "source": [
    "## 2. Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6eb411b-67a5-4f90-addc-1fe49c741838",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef62350c-3ed9-4b54-a69b-a3e93059c550",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [based, on, additional, information, received,...\n",
       "1    [based, on, additional, information, received,...\n",
       "2    [if, information, is, provided, in, the, futur...\n",
       "3    [manufacturer, reference, number, b4, incident...\n",
       "4    [the, patients, attorney, alleged, a, deficien...\n",
       "Name: NOPUNCT_TEXT, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to remove punctuation in the data\n",
    "def remove_punctuation(text):\n",
    "    text = \"\".join(\n",
    "        [character for character in text if character not in string.punctuation]\n",
    "    )\n",
    "    return text\n",
    "\n",
    "\n",
    "df[\"NOPUNCT_TEXT\"] = df[\"TOKENIZED_TEXT\"].apply(\n",
    "    lambda x: [remove_punctuation(word) for word in x]\n",
    ")\n",
    "df[\"NOPUNCT_TEXT\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fe886e3-fad3-4ca4-a02a-d94b796eb33a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['manufacturer',\n",
       " 'reference',\n",
       " 'number',\n",
       " 'b4',\n",
       " 'incident',\n",
       " 'date',\n",
       " 'was',\n",
       " 'not',\n",
       " 'provided',\n",
       " 'lot',\n",
       " 'number',\n",
       " 'not',\n",
       " 'provided',\n",
       " 'udi',\n",
       " 'not',\n",
       " 'provided',\n",
       " '',\n",
       " 'reprocessing',\n",
       " 'information',\n",
       " 'not',\n",
       " 'provided',\n",
       " 'since',\n",
       " 'the',\n",
       " 'lot',\n",
       " 'number',\n",
       " 'was',\n",
       " 'not',\n",
       " 'provided',\n",
       " 'this',\n",
       " 'information',\n",
       " 'cannot',\n",
       " 'be',\n",
       " 'determined']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"NOPUNCT_TEXT\"][verification_row]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c01067-455b-44af-95ca-93854f5cc469",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cebbe10-e09c-4aa5-917d-5159fb44be56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [based, additional, information, received, com...\n",
       "1    [based, additional, information, received, com...\n",
       "2    [information, provided, future, supplemental, ...\n",
       "3    [manufacturer, reference, number, b4, incident...\n",
       "4    [patients, attorney, alleged, deficiency, devi...\n",
       "Name: NOSTOPWORDS_TEXT, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "\n",
    "# Define a function to convert to lowercase and remove stopwords\n",
    "def remove_stopwords(tokenized_text):\n",
    "    text = [word for word in tokenized_text if word.lower() not in stopwords]\n",
    "    return text\n",
    "\n",
    "\n",
    "df[\"NOSTOPWORDS_TEXT\"] = df[\"NOPUNCT_TEXT\"].apply(lambda x: remove_stopwords(x))\n",
    "df[\"NOSTOPWORDS_TEXT\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06cf0ce2-c8fc-4376-86d3-19c9dcdb31ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['manufacturer',\n",
       " 'reference',\n",
       " 'number',\n",
       " 'b4',\n",
       " 'incident',\n",
       " 'date',\n",
       " 'provided',\n",
       " 'lot',\n",
       " 'number',\n",
       " 'provided',\n",
       " 'udi',\n",
       " 'provided',\n",
       " '',\n",
       " 'reprocessing',\n",
       " 'information',\n",
       " 'provided',\n",
       " 'since',\n",
       " 'lot',\n",
       " 'number',\n",
       " 'provided',\n",
       " 'information',\n",
       " 'cannot',\n",
       " 'determined']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"NOSTOPWORDS_TEXT\"][verification_row]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f671ef-1c12-4e60-a58e-9007e4ace32a",
   "metadata": {},
   "source": [
    "## 4. Remove Words Starting with a Digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "202dfd97-3eb9-4bf8-8cc2-3762a0bf8ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [based, additional, information, received, com...\n",
       "1    [based, additional, information, received, com...\n",
       "2    [information, provided, future, supplemental, ...\n",
       "3    [manufacturer, reference, number, b4, incident...\n",
       "4    [patients, attorney, alleged, deficiency, devi...\n",
       "Name: NODIGITS_TEXT, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# create a regular expression pattern to match words that start with numbers\n",
    "pattern = re.compile(r\"^\\d+\")\n",
    "\n",
    "\n",
    "# Define a function to convert to lowercase and remove stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    text = [word for word in tokens if not pattern.match(word)]\n",
    "    return text\n",
    "\n",
    "\n",
    "df[\"NODIGITS_TEXT\"] = df[\"NOSTOPWORDS_TEXT\"].apply(lambda x: remove_stopwords(x))\n",
    "df[\"NODIGITS_TEXT\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c663980d-2329-4dfe-ae08-5a5057fc2185",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['manufacturer',\n",
       " 'reference',\n",
       " 'number',\n",
       " 'b4',\n",
       " 'incident',\n",
       " 'date',\n",
       " 'provided',\n",
       " 'lot',\n",
       " 'number',\n",
       " 'provided',\n",
       " 'udi',\n",
       " 'provided',\n",
       " '',\n",
       " 'reprocessing',\n",
       " 'information',\n",
       " 'provided',\n",
       " 'since',\n",
       " 'lot',\n",
       " 'number',\n",
       " 'provided',\n",
       " 'information',\n",
       " 'cannot',\n",
       " 'determined']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"NODIGITS_TEXT\"][verification_row]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1fe705-642f-44fd-952b-90011bdb4a06",
   "metadata": {},
   "source": [
    "## X. Word Frequency Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36a4a4da-960f-4cfb-ac35-928c7937e443",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>patient</td>\n",
       "      <td>30407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pain</td>\n",
       "      <td>19251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b6</td>\n",
       "      <td>17037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>device</td>\n",
       "      <td>16064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>reported</td>\n",
       "      <td>12668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13330</th>\n",
       "      <td>payers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13331</th>\n",
       "      <td>submucosally</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13332</th>\n",
       "      <td>maneuvered</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13333</th>\n",
       "      <td>deviating</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13334</th>\n",
       "      <td>clonazapan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13335 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Word  Frequency\n",
       "0           patient      30407\n",
       "1              pain      19251\n",
       "2                b6      17037\n",
       "3            device      16064\n",
       "4          reported      12668\n",
       "...             ...        ...\n",
       "13330        payers          1\n",
       "13331  submucosally          1\n",
       "13332    maneuvered          1\n",
       "13333     deviating          1\n",
       "13334    clonazapan          1\n",
       "\n",
       "[13335 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explode the lists into separate rows\n",
    "exploded_df = df.explode(\"NODIGITS_TEXT\")\n",
    "word_freq = exploded_df[\"NODIGITS_TEXT\"].value_counts()\n",
    "\n",
    "# Create a DataFrame from the word frequency data\n",
    "freq_df = pd.DataFrame({\"Word\": word_freq.index, \"Frequency\": word_freq.values})\n",
    "freq_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c213aa-997b-4371-8dd9-c6b5f5dfe07e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Parts of Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e318ac0-d7d2-406b-b2df-6cb861548bd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [(based, VBN), (additional, JJ), (information,...\n",
       "1    [(based, VBN), (additional, JJ), (information,...\n",
       "2    [(information, NN), (provided, VBD), (future, ...\n",
       "3    [(manufacturer, NN), (reference, NN), (number,...\n",
       "4    [(patients, NNS), (attorney, NN), (alleged, VB...\n",
       "Name: POS_TEXT, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nltk.pos_tag() function to each row of the TOKENIZED_TEXT column\n",
    "# pos_tag returns a Tuple for each word consisting of the word and its classification\n",
    "# TODO: List classifications and their abbreviations\n",
    "df[\"POS_TEXT\"] = df[\"NODIGITS_TEXT\"].apply(nltk.pos_tag)\n",
    "df[\"POS_TEXT\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12140ece-4515-4637-8efe-dfb3cfa8cf3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('patients', 'NNS'),\n",
       " ('attorney', 'NN'),\n",
       " ('alleged', 'VBN'),\n",
       " ('deficiency', 'NN'),\n",
       " ('device', 'NN'),\n",
       " ('resulting', 'VBG'),\n",
       " ('unspecified', 'JJ'),\n",
       " ('adverse', 'JJ'),\n",
       " ('outcome', 'NN'),\n",
       " ('product', 'NN'),\n",
       " ('used', 'VBN'),\n",
       " ('therapeutic', 'JJ'),\n",
       " ('treatment', 'NN'),\n",
       " ('preoperative', 'JJ'),\n",
       " ('postoperative', 'JJ'),\n",
       " ('diagnosis', 'NN'),\n",
       " ('stress', 'NN'),\n",
       " ('urinary', 'JJ'),\n",
       " ('incontinence', 'NN'),\n",
       " ('procedure', 'NN'),\n",
       " ('performed', 'VBD'),\n",
       " ('transvaginal', 'JJ'),\n",
       " ('sling', 'NN'),\n",
       " ('placement', 'NN'),\n",
       " ('patient', 'NN'),\n",
       " ('returned', 'VBD'),\n",
       " ('office', 'NN'),\n",
       " ('visit', 'NN'),\n",
       " ('b6', 'NN'),\n",
       " ('patient', 'NN'),\n",
       " ('complained', 'VBD'),\n",
       " ('incontinence', 'NN'),\n",
       " ('stream', 'NN'),\n",
       " ('started', 'VBD'),\n",
       " ('strong', 'JJ'),\n",
       " ('began', 'VBD'),\n",
       " ('weaken', 'JJ'),\n",
       " ('stated', 'VBN'),\n",
       " ('would', 'MD'),\n",
       " ('stop', 'VB'),\n",
       " ('start', 'NN'),\n",
       " ('stood', 'VBD'),\n",
       " ('voiding', 'VBG'),\n",
       " ('began', 'VBD'),\n",
       " ('leak', 'JJ'),\n",
       " ('immediately', 'RB'),\n",
       " ('constantly', 'RB'),\n",
       " ('wear', 'JJ'),\n",
       " ('pads', 'NNS'),\n",
       " ('also', 'RB'),\n",
       " ('complained', 'VBD'),\n",
       " ('leaking', 'VBG'),\n",
       " ('without', 'IN'),\n",
       " ('every', 'DT'),\n",
       " ('feeling', 'NN'),\n",
       " ('need', 'NN'),\n",
       " ('urinate', 'JJ'),\n",
       " ('assessment', 'NN'),\n",
       " ('continuous', 'JJ'),\n",
       " ('leakage', 'NN'),\n",
       " ('urge', 'NN'),\n",
       " ('incontinence', 'NN'),\n",
       " ('mixed', 'JJ'),\n",
       " ('incontinence', 'NN'),\n",
       " ('patient', 'JJ'),\n",
       " ('underwent', 'JJ'),\n",
       " ('cystoscopy', 'NN'),\n",
       " ('b6', 'NN'),\n",
       " ('patient', 'NN'),\n",
       " ('returned', 'VBD'),\n",
       " ('office', 'NN'),\n",
       " ('visit', 'NN'),\n",
       " ('b6', 'NN'),\n",
       " ('patient', 'NN'),\n",
       " ('presented', 'VBD'),\n",
       " ('complaints', 'NNS'),\n",
       " ('vaginal', 'JJ'),\n",
       " ('obstruction', 'NN'),\n",
       " ('reported', 'VBD'),\n",
       " ('something', 'NN'),\n",
       " ('fallen', 'VBN'),\n",
       " ('vagina', 'JJ'),\n",
       " ('assessment', 'NN'),\n",
       " ('continuous', 'JJ'),\n",
       " ('leakage', 'NN'),\n",
       " ('urge', 'NN'),\n",
       " ('incontinence', 'NN'),\n",
       " ('mixed', 'JJ'),\n",
       " ('incontinence', 'NN'),\n",
       " ('patient', 'NN'),\n",
       " ('returned', 'VBD'),\n",
       " ('office', 'NN'),\n",
       " ('visit', 'NN'),\n",
       " ('b6', 'NN'),\n",
       " ('stated', 'VBN'),\n",
       " ('leakage', 'NN'),\n",
       " ('without', 'IN'),\n",
       " ('urge', 'JJ'),\n",
       " ('patient', 'JJ'),\n",
       " ('underwent', 'JJ'),\n",
       " ('additional', 'JJ'),\n",
       " ('procedure', 'NN'),\n",
       " ('b6', 'NN'),\n",
       " ('preoperative', 'JJ'),\n",
       " ('postoperative', 'JJ'),\n",
       " ('diagnosis', 'NN'),\n",
       " ('stress', 'NN'),\n",
       " ('urinary', 'JJ'),\n",
       " ('incontinence', 'NN'),\n",
       " ('procedure', 'NN'),\n",
       " ('performed', 'VBD'),\n",
       " ('autologous', 'JJ'),\n",
       " ('fascial', 'JJ'),\n",
       " ('sling', 'VBG'),\n",
       " ('cystoscopy', 'NN'),\n",
       " ('patient', 'NN'),\n",
       " ('returned', 'VBD'),\n",
       " ('office', 'NN'),\n",
       " ('visit', 'NN'),\n",
       " ('b6', 'NN'),\n",
       " ('reported', 'VBD'),\n",
       " ('seen', 'VBN'),\n",
       " ('improvement', 'NN'),\n",
       " ('stress', 'NN'),\n",
       " ('leakage', 'NN'),\n",
       " ('however', 'RB'),\n",
       " ('urge', 'JJ'),\n",
       " ('leakage', 'NN'),\n",
       " ('still', 'RB'),\n",
       " ('big', 'JJ'),\n",
       " ('problem', 'NN'),\n",
       " ('stated', 'VBD'),\n",
       " ('overall', 'JJ'),\n",
       " ('feeling', 'NN'),\n",
       " ('well', 'RB'),\n",
       " ('required', 'JJ'),\n",
       " ('pain', 'NN'),\n",
       " ('medication', 'NN')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"POS_TEXT\"][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462e6d3f-d2a6-414b-ae7a-55481ba8d5fe",
   "metadata": {},
   "source": [
    "## 6. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "034f05dc-490e-4692-b1cb-2445732f80d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [base, additional, information, receive, compl...\n",
       "1    [base, additional, information, receive, compl...\n",
       "2    [information, provide, future, supplemental, r...\n",
       "3    [manufacturer, reference, number, b4, incident...\n",
       "4    [patient, attorney, allege, deficiency, device...\n",
       "Name: LEMMATIZED_TEXT, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# define a function to lemmatize each word in a text list based on its POS tag\n",
    "def lemmatize_text(pos_tagged_text):\n",
    "    # initialize WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # map NLTK's POS tags to WordNet's POS tags\n",
    "    # TODO: list the abbreviations for WordNet's parts of speech\n",
    "    pos_map = {\"N\": \"n\", \"V\": \"v\", \"R\": \"r\", \"J\": \"a\"}\n",
    "\n",
    "    # lemmatize each word in the text list based on its POS tag\n",
    "    lemmatized_text = []\n",
    "\n",
    "    for word, pos in pos_tagged_text:\n",
    "        # get the first character of the POS tag to use as the WordNet POS tag\n",
    "        #\n",
    "        # Set the WordNetLemmatizer default to Nouns ('n') or Verbs ('v')\n",
    "        #\n",
    "        wn_pos = pos_map.get(pos[0], \"n\")\n",
    "\n",
    "        # lemmatize the word and append it to the lemmatized text list\n",
    "        lemmatized_word = lemmatizer.lemmatize(word, pos=wn_pos)\n",
    "        lemmatized_text.append(lemmatized_word)\n",
    "\n",
    "    # return the lemmatized text list\n",
    "    return lemmatized_text\n",
    "\n",
    "\n",
    "# apply the lemmatize_text function to each row of the dataframe\n",
    "df[\"LEMMATIZED_TEXT\"] = df[\"POS_TEXT\"].apply(lemmatize_text)\n",
    "df[\"LEMMATIZED_TEXT\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66e4cfe-298d-4186-ab98-74848658af20",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 7. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6db2894c-4f23-4d9c-9b0f-ef344c9db60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [base, addit, inform, receiv, complaint, medtr...\n",
       "1    [base, addit, inform, receiv, complaint, medtr...\n",
       "2    [inform, provid, futur, supplement, report, issu]\n",
       "3    [manufactur, refer, number, b4, incid, date, p...\n",
       "4    [patient, attorney, alleg, defici, devic, resu...\n",
       "Name: STEMMED_TEXT, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# define a function to stem each word in a text list\n",
    "def stem_words(pos_tagged_text):\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    stemmed_text = []\n",
    "\n",
    "    for word, pos in pos_tagged_text:\n",
    "        # stem the word and append it to the stemmed text list\n",
    "        stemmed_word = stemmer.stem(word)\n",
    "        stemmed_text.append(stemmed_word)\n",
    "\n",
    "    # return the stemmed text list\n",
    "    return stemmed_text\n",
    "\n",
    "\n",
    "df[\"STEMMED_TEXT\"] = df[\"POS_TEXT\"].apply(stem_words)\n",
    "df[\"STEMMED_TEXT\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23bed8a-188b-4b6d-ba1b-6d17911dec96",
   "metadata": {},
   "source": [
    "## Compare the results of lemmatization and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9f7eda1-d616-419f-990c-ec13f885ea56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_7f666 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_7f666_row0_col0, #T_7f666_row0_col1, #T_7f666_row0_col2, #T_7f666_row1_col0, #T_7f666_row1_col1, #T_7f666_row1_col2, #T_7f666_row2_col0, #T_7f666_row2_col1, #T_7f666_row2_col2, #T_7f666_row3_col0, #T_7f666_row3_col1, #T_7f666_row3_col2, #T_7f666_row4_col0, #T_7f666_row4_col1, #T_7f666_row4_col2, #T_7f666_row5_col0, #T_7f666_row5_col1, #T_7f666_row5_col2, #T_7f666_row6_col0, #T_7f666_row6_col1, #T_7f666_row6_col2, #T_7f666_row7_col0, #T_7f666_row7_col1, #T_7f666_row7_col2, #T_7f666_row8_col0, #T_7f666_row8_col1, #T_7f666_row8_col2, #T_7f666_row9_col0, #T_7f666_row9_col1, #T_7f666_row9_col2, #T_7f666_row10_col0, #T_7f666_row10_col1, #T_7f666_row10_col2, #T_7f666_row11_col0, #T_7f666_row11_col1, #T_7f666_row11_col2, #T_7f666_row12_col0, #T_7f666_row12_col1, #T_7f666_row12_col2, #T_7f666_row13_col0, #T_7f666_row13_col1, #T_7f666_row13_col2, #T_7f666_row14_col0, #T_7f666_row14_col1, #T_7f666_row14_col2, #T_7f666_row15_col0, #T_7f666_row15_col1, #T_7f666_row15_col2, #T_7f666_row16_col0, #T_7f666_row16_col1, #T_7f666_row16_col2, #T_7f666_row17_col0, #T_7f666_row17_col1, #T_7f666_row17_col2, #T_7f666_row18_col0, #T_7f666_row18_col1, #T_7f666_row18_col2, #T_7f666_row19_col0, #T_7f666_row19_col1, #T_7f666_row19_col2, #T_7f666_row20_col0, #T_7f666_row20_col1, #T_7f666_row20_col2, #T_7f666_row21_col0, #T_7f666_row21_col1, #T_7f666_row21_col2, #T_7f666_row22_col0, #T_7f666_row22_col1, #T_7f666_row22_col2 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_7f666\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_7f666_level0_col0\" class=\"col_heading level0 col0\" >WORD, PART OF SPEECH</th>\n",
       "      <th id=\"T_7f666_level0_col1\" class=\"col_heading level0 col1\" >LEMMA</th>\n",
       "      <th id=\"T_7f666_level0_col2\" class=\"col_heading level0 col2\" >STEM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_7f666_row0_col0\" class=\"data row0 col0\" >('manufacturer', 'NN')</td>\n",
       "      <td id=\"T_7f666_row0_col1\" class=\"data row0 col1\" >manufacturer</td>\n",
       "      <td id=\"T_7f666_row0_col2\" class=\"data row0 col2\" >manufactur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_7f666_row1_col0\" class=\"data row1 col0\" >('reference', 'NN')</td>\n",
       "      <td id=\"T_7f666_row1_col1\" class=\"data row1 col1\" >reference</td>\n",
       "      <td id=\"T_7f666_row1_col2\" class=\"data row1 col2\" >refer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_7f666_row2_col0\" class=\"data row2 col0\" >('number', 'NN')</td>\n",
       "      <td id=\"T_7f666_row2_col1\" class=\"data row2 col1\" >number</td>\n",
       "      <td id=\"T_7f666_row2_col2\" class=\"data row2 col2\" >number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_7f666_row3_col0\" class=\"data row3 col0\" >('b4', 'JJ')</td>\n",
       "      <td id=\"T_7f666_row3_col1\" class=\"data row3 col1\" >b4</td>\n",
       "      <td id=\"T_7f666_row3_col2\" class=\"data row3 col2\" >b4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_7f666_row4_col0\" class=\"data row4 col0\" >('incident', 'JJ')</td>\n",
       "      <td id=\"T_7f666_row4_col1\" class=\"data row4 col1\" >incident</td>\n",
       "      <td id=\"T_7f666_row4_col2\" class=\"data row4 col2\" >incid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_7f666_row5_col0\" class=\"data row5 col0\" >('date', 'NN')</td>\n",
       "      <td id=\"T_7f666_row5_col1\" class=\"data row5 col1\" >date</td>\n",
       "      <td id=\"T_7f666_row5_col2\" class=\"data row5 col2\" >date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_7f666_row6_col0\" class=\"data row6 col0\" >('provided', 'VBD')</td>\n",
       "      <td id=\"T_7f666_row6_col1\" class=\"data row6 col1\" >provide</td>\n",
       "      <td id=\"T_7f666_row6_col2\" class=\"data row6 col2\" >provid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_7f666_row7_col0\" class=\"data row7 col0\" >('lot', 'NN')</td>\n",
       "      <td id=\"T_7f666_row7_col1\" class=\"data row7 col1\" >lot</td>\n",
       "      <td id=\"T_7f666_row7_col2\" class=\"data row7 col2\" >lot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_7f666_row8_col0\" class=\"data row8 col0\" >('number', 'NN')</td>\n",
       "      <td id=\"T_7f666_row8_col1\" class=\"data row8 col1\" >number</td>\n",
       "      <td id=\"T_7f666_row8_col2\" class=\"data row8 col2\" >number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_7f666_row9_col0\" class=\"data row9 col0\" >('provided', 'VBD')</td>\n",
       "      <td id=\"T_7f666_row9_col1\" class=\"data row9 col1\" >provide</td>\n",
       "      <td id=\"T_7f666_row9_col2\" class=\"data row9 col2\" >provid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_7f666_row10_col0\" class=\"data row10 col0\" >('udi', 'JJ')</td>\n",
       "      <td id=\"T_7f666_row10_col1\" class=\"data row10 col1\" >udi</td>\n",
       "      <td id=\"T_7f666_row10_col2\" class=\"data row10 col2\" >udi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_7f666_row11_col0\" class=\"data row11 col0\" >('provided', 'VBN')</td>\n",
       "      <td id=\"T_7f666_row11_col1\" class=\"data row11 col1\" >provide</td>\n",
       "      <td id=\"T_7f666_row11_col2\" class=\"data row11 col2\" >provid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_7f666_row12_col0\" class=\"data row12 col0\" >('', 'JJ')</td>\n",
       "      <td id=\"T_7f666_row12_col1\" class=\"data row12 col1\" ></td>\n",
       "      <td id=\"T_7f666_row12_col2\" class=\"data row12 col2\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_7f666_row13_col0\" class=\"data row13 col0\" >('reprocessing', 'VBG')</td>\n",
       "      <td id=\"T_7f666_row13_col1\" class=\"data row13 col1\" >reprocess</td>\n",
       "      <td id=\"T_7f666_row13_col2\" class=\"data row13 col2\" >reprocess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_7f666_row14_col0\" class=\"data row14 col0\" >('information', 'NN')</td>\n",
       "      <td id=\"T_7f666_row14_col1\" class=\"data row14 col1\" >information</td>\n",
       "      <td id=\"T_7f666_row14_col2\" class=\"data row14 col2\" >inform</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_7f666_row15_col0\" class=\"data row15 col0\" >('provided', 'VBN')</td>\n",
       "      <td id=\"T_7f666_row15_col1\" class=\"data row15 col1\" >provide</td>\n",
       "      <td id=\"T_7f666_row15_col2\" class=\"data row15 col2\" >provid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_7f666_row16_col0\" class=\"data row16 col0\" >('since', 'IN')</td>\n",
       "      <td id=\"T_7f666_row16_col1\" class=\"data row16 col1\" >since</td>\n",
       "      <td id=\"T_7f666_row16_col2\" class=\"data row16 col2\" >sinc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_7f666_row17_col0\" class=\"data row17 col0\" >('lot', 'NN')</td>\n",
       "      <td id=\"T_7f666_row17_col1\" class=\"data row17 col1\" >lot</td>\n",
       "      <td id=\"T_7f666_row17_col2\" class=\"data row17 col2\" >lot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_7f666_row18_col0\" class=\"data row18 col0\" >('number', 'NN')</td>\n",
       "      <td id=\"T_7f666_row18_col1\" class=\"data row18 col1\" >number</td>\n",
       "      <td id=\"T_7f666_row18_col2\" class=\"data row18 col2\" >number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_7f666_row19_col0\" class=\"data row19 col0\" >('provided', 'VBN')</td>\n",
       "      <td id=\"T_7f666_row19_col1\" class=\"data row19 col1\" >provide</td>\n",
       "      <td id=\"T_7f666_row19_col2\" class=\"data row19 col2\" >provid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_7f666_row20_col0\" class=\"data row20 col0\" >('information', 'NN')</td>\n",
       "      <td id=\"T_7f666_row20_col1\" class=\"data row20 col1\" >information</td>\n",
       "      <td id=\"T_7f666_row20_col2\" class=\"data row20 col2\" >inform</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_7f666_row21_col0\" class=\"data row21 col0\" >('cannot', 'NN')</td>\n",
       "      <td id=\"T_7f666_row21_col1\" class=\"data row21 col1\" >cannot</td>\n",
       "      <td id=\"T_7f666_row21_col2\" class=\"data row21 col2\" >cannot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_7f666_row22_col0\" class=\"data row22 col0\" >('determined', 'VBD')</td>\n",
       "      <td id=\"T_7f666_row22_col1\" class=\"data row22 col1\" >determine</td>\n",
       "      <td id=\"T_7f666_row22_col2\" class=\"data row22 col2\" >determin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x11f0627a0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_lemma_stem_df = pd.DataFrame(\n",
    "    {\n",
    "        \"WORD, PART OF SPEECH\": df[\"POS_TEXT\"][verification_row],\n",
    "        \"LEMMA\": df[\"LEMMATIZED_TEXT\"][verification_row],\n",
    "        \"STEM\": df[\"STEMMED_TEXT\"][verification_row],\n",
    "    }\n",
    ")\n",
    "\n",
    "compare_lemma_stem_df = compare_lemma_stem_df.style.set_properties(\n",
    "    **{\"text-align\": \"left\"}\n",
    ")\n",
    "compare_lemma_stem_df = compare_lemma_stem_df.set_table_styles(\n",
    "    [dict(selector=\"th\", props=[(\"text-align\", \"left\")])]\n",
    ")\n",
    "compare_lemma_stem_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3080c448-ef79-4bfa-bf12-884a043b4eb2",
   "metadata": {},
   "source": [
    "## 8. Create Bag of Words (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb023fab-c436-4827-a86a-6b15bd24bfe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 7. Create Bag of Words (BOW)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# create a CountVectorizer object\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# fit the vectorizer to the text data\n",
    "count_vectorizer.fit(df[\"LEMMATIZED_TEXT\"].apply(lambda x: \" \".join(x)))\n",
    "\n",
    "# create a bag of words matrix\n",
    "bow_matrix = count_vectorizer.transform(\n",
    "    df[\"LEMMATIZED_TEXT\"].apply(lambda x: \" \".join(x))\n",
    ")\n",
    "\n",
    "# convert the bag of words matrix to a DataFrame\n",
    "bow_df = pd.DataFrame(\n",
    "    bow_matrix.toarray(), columns=count_vectorizer.get_feature_names_out()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fdabd803-b610-47a5-95dc-3d83c0be0d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9605, 11671)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b63a7c3b-84bd-4c92-a79b-2d1b00ce7c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abbott</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>abdominal</th>\n",
       "      <th>aberration</th>\n",
       "      <th>able</th>\n",
       "      <th>accessory</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accurate</th>\n",
       "      <th>acetaminophen</th>\n",
       "      <th>actually</th>\n",
       "      <th>...</th>\n",
       "      <th>work</th>\n",
       "      <th>would</th>\n",
       "      <th>x2</th>\n",
       "      <th>xray</th>\n",
       "      <th>year</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>zero</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 922 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abbott  abdomen  abdominal  aberration  able  accessory  accuracy  \\\n",
       "0       0        0          0           0     0          0         0   \n",
       "1       0        0          0           0     0          0         0   \n",
       "2       0        0          0           0     0          0         0   \n",
       "3       0        0          0           0     0          0         0   \n",
       "4       0        0          0           0     0          0         0   \n",
       "\n",
       "   accurate  acetaminophen  actually  ...  work  would  x2  xray  year  \\\n",
       "0         0              0         0  ...     0      0   0     0     0   \n",
       "1         0              0         0  ...     0      0   0     0     0   \n",
       "2         0              0         0  ...     0      0   0     0     0   \n",
       "3         0              0         0  ...     0      0   0     0     0   \n",
       "4         0              0         0  ...     0      0   0     0     0   \n",
       "\n",
       "   yellow  yes  yet  zero  zone  \n",
       "0       0    0    0     0     0  \n",
       "1       0    0    0     0     0  \n",
       "2       0    0    0     0     0  \n",
       "3       0    0    0     0     0  \n",
       "4       0    0    0     0     0  \n",
       "\n",
       "[5 rows x 922 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_df.head()\n",
    "# TODO: Plot the BOW results (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a68c5c-55bc-4559-af40-e8ae3ad76d5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 9. Calculate Term Frequency-Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5fc601b8-7e4a-4a15-aa80-7017721a7892",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 8. Calculate Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create a CountVectorizer object and fit it to the text data\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "X = tfidf_vectorizer.fit_transform(df[\"LEMMATIZED_TEXT\"].apply(lambda x: \" \".join(x)))\n",
    "\n",
    "# convert the sparse matrix to a DataFrame\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc7dc764-c475-4da1-9a5e-62d3a103688a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9605, 11671)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ced8af5a-9c8a-4e5f-8bf6-c4d0b9a2ab68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a020503</th>\n",
       "      <th>a04</th>\n",
       "      <th>a040609</th>\n",
       "      <th>a0414</th>\n",
       "      <th>a050</th>\n",
       "      <th>a0501</th>\n",
       "      <th>a1</th>\n",
       "      <th>a1502</th>\n",
       "      <th>a150201</th>\n",
       "      <th>a150205</th>\n",
       "      <th>...</th>\n",
       "      <th>zoloftnow</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoref</th>\n",
       "      <th>zoster</th>\n",
       "      <th>zosyn</th>\n",
       "      <th>zsi</th>\n",
       "      <th>zuban</th>\n",
       "      <th>zyprexa</th>\n",
       "      <th>zyrtec</th>\n",
       "      <th>zyson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 11671 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   a020503  a04  a040609  a0414  a050  a0501   a1  a1502  a150201  a150205  \\\n",
       "0      0.0  0.0      0.0    0.0   0.0    0.0  0.0    0.0      0.0      0.0   \n",
       "1      0.0  0.0      0.0    0.0   0.0    0.0  0.0    0.0      0.0      0.0   \n",
       "\n",
       "   ...  zoloftnow  zone  zoref  zoster  zosyn  zsi  zuban  zyprexa  zyrtec  \\\n",
       "0  ...        0.0   0.0    0.0     0.0    0.0  0.0    0.0      0.0     0.0   \n",
       "1  ...        0.0   0.0    0.0     0.0    0.0  0.0    0.0      0.0     0.0   \n",
       "\n",
       "   zyson  \n",
       "0    0.0  \n",
       "1    0.0  \n",
       "\n",
       "[2 rows x 11671 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0e2ab0-8c85-4795-868a-603b77154cca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 10. Sentencize\n",
    "The `FOI_TEXT` can be processed as sentences.\n",
    "\n",
    "For further analysis, each sentence needs to be associated with the `FOI_TEXT` row that it came from.\n",
    "\n",
    "[This discussion from Stack Overflow](https://stackoverflow.com/a/43922444/2308522) provides a suggestion for breaking the code into a dataframe of sentences with each sentence retaining the ID of the row where it was originally located.\n",
    "\n",
    "[This page from the Pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.itertuples.html) provides details on using the `itertuples()` function to process the rows of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c04dab76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASED ON ADDITIONAL INFORMATION RECEIVED THIS COMPLAINT IS NOT A MEDTRONIC PRODUCT. IF INFORMATION IS PROVIDED IN THE FUTURE, A SUPPLEMENTAL REPORT WILL BE ISSUED.\n"
     ]
    }
   ],
   "source": [
    "for row in df.itertuples():\n",
    "    print(row[7])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2a577254-5ecb-4a76-8d42-3021092ca551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_7f666 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_7f666_row0_col0, #T_7f666_row0_col1, #T_7f666_row0_col2, #T_7f666_row1_col0, #T_7f666_row1_col1, #T_7f666_row1_col2, #T_7f666_row2_col0, #T_7f666_row2_col1, #T_7f666_row2_col2, #T_7f666_row3_col0, #T_7f666_row3_col1, #T_7f666_row3_col2, #T_7f666_row4_col0, #T_7f666_row4_col1, #T_7f666_row4_col2, #T_7f666_row5_col0, #T_7f666_row5_col1, #T_7f666_row5_col2, #T_7f666_row6_col0, #T_7f666_row6_col1, #T_7f666_row6_col2, #T_7f666_row7_col0, #T_7f666_row7_col1, #T_7f666_row7_col2, #T_7f666_row8_col0, #T_7f666_row8_col1, #T_7f666_row8_col2, #T_7f666_row9_col0, #T_7f666_row9_col1, #T_7f666_row9_col2, #T_7f666_row10_col0, #T_7f666_row10_col1, #T_7f666_row10_col2, #T_7f666_row11_col0, #T_7f666_row11_col1, #T_7f666_row11_col2, #T_7f666_row12_col0, #T_7f666_row12_col1, #T_7f666_row12_col2, #T_7f666_row13_col0, #T_7f666_row13_col1, #T_7f666_row13_col2, #T_7f666_row14_col0, #T_7f666_row14_col1, #T_7f666_row14_col2, #T_7f666_row15_col0, #T_7f666_row15_col1, #T_7f666_row15_col2, #T_7f666_row16_col0, #T_7f666_row16_col1, #T_7f666_row16_col2, #T_7f666_row17_col0, #T_7f666_row17_col1, #T_7f666_row17_col2, #T_7f666_row18_col0, #T_7f666_row18_col1, #T_7f666_row18_col2, #T_7f666_row19_col0, #T_7f666_row19_col1, #T_7f666_row19_col2, #T_7f666_row20_col0, #T_7f666_row20_col1, #T_7f666_row20_col2, #T_7f666_row21_col0, #T_7f666_row21_col1, #T_7f666_row21_col2, #T_7f666_row22_col0, #T_7f666_row22_col1, #T_7f666_row22_col2 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_7f666\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_7f666_level0_col0\" class=\"col_heading level0 col0\" >WORD, PART OF SPEECH</th>\n",
       "      <th id=\"T_7f666_level0_col1\" class=\"col_heading level0 col1\" >LEMMA</th>\n",
       "      <th id=\"T_7f666_level0_col2\" class=\"col_heading level0 col2\" >STEM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_7f666_row0_col0\" class=\"data row0 col0\" >('manufacturer', 'NN')</td>\n",
       "      <td id=\"T_7f666_row0_col1\" class=\"data row0 col1\" >manufacturer</td>\n",
       "      <td id=\"T_7f666_row0_col2\" class=\"data row0 col2\" >manufactur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_7f666_row1_col0\" class=\"data row1 col0\" >('reference', 'NN')</td>\n",
       "      <td id=\"T_7f666_row1_col1\" class=\"data row1 col1\" >reference</td>\n",
       "      <td id=\"T_7f666_row1_col2\" class=\"data row1 col2\" >refer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_7f666_row2_col0\" class=\"data row2 col0\" >('number', 'NN')</td>\n",
       "      <td id=\"T_7f666_row2_col1\" class=\"data row2 col1\" >number</td>\n",
       "      <td id=\"T_7f666_row2_col2\" class=\"data row2 col2\" >number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_7f666_row3_col0\" class=\"data row3 col0\" >('b4', 'JJ')</td>\n",
       "      <td id=\"T_7f666_row3_col1\" class=\"data row3 col1\" >b4</td>\n",
       "      <td id=\"T_7f666_row3_col2\" class=\"data row3 col2\" >b4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_7f666_row4_col0\" class=\"data row4 col0\" >('incident', 'JJ')</td>\n",
       "      <td id=\"T_7f666_row4_col1\" class=\"data row4 col1\" >incident</td>\n",
       "      <td id=\"T_7f666_row4_col2\" class=\"data row4 col2\" >incid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_7f666_row5_col0\" class=\"data row5 col0\" >('date', 'NN')</td>\n",
       "      <td id=\"T_7f666_row5_col1\" class=\"data row5 col1\" >date</td>\n",
       "      <td id=\"T_7f666_row5_col2\" class=\"data row5 col2\" >date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_7f666_row6_col0\" class=\"data row6 col0\" >('provided', 'VBD')</td>\n",
       "      <td id=\"T_7f666_row6_col1\" class=\"data row6 col1\" >provide</td>\n",
       "      <td id=\"T_7f666_row6_col2\" class=\"data row6 col2\" >provid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_7f666_row7_col0\" class=\"data row7 col0\" >('lot', 'NN')</td>\n",
       "      <td id=\"T_7f666_row7_col1\" class=\"data row7 col1\" >lot</td>\n",
       "      <td id=\"T_7f666_row7_col2\" class=\"data row7 col2\" >lot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_7f666_row8_col0\" class=\"data row8 col0\" >('number', 'NN')</td>\n",
       "      <td id=\"T_7f666_row8_col1\" class=\"data row8 col1\" >number</td>\n",
       "      <td id=\"T_7f666_row8_col2\" class=\"data row8 col2\" >number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_7f666_row9_col0\" class=\"data row9 col0\" >('provided', 'VBD')</td>\n",
       "      <td id=\"T_7f666_row9_col1\" class=\"data row9 col1\" >provide</td>\n",
       "      <td id=\"T_7f666_row9_col2\" class=\"data row9 col2\" >provid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_7f666_row10_col0\" class=\"data row10 col0\" >('udi', 'JJ')</td>\n",
       "      <td id=\"T_7f666_row10_col1\" class=\"data row10 col1\" >udi</td>\n",
       "      <td id=\"T_7f666_row10_col2\" class=\"data row10 col2\" >udi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_7f666_row11_col0\" class=\"data row11 col0\" >('provided', 'VBN')</td>\n",
       "      <td id=\"T_7f666_row11_col1\" class=\"data row11 col1\" >provide</td>\n",
       "      <td id=\"T_7f666_row11_col2\" class=\"data row11 col2\" >provid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_7f666_row12_col0\" class=\"data row12 col0\" >('', 'JJ')</td>\n",
       "      <td id=\"T_7f666_row12_col1\" class=\"data row12 col1\" ></td>\n",
       "      <td id=\"T_7f666_row12_col2\" class=\"data row12 col2\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_7f666_row13_col0\" class=\"data row13 col0\" >('reprocessing', 'VBG')</td>\n",
       "      <td id=\"T_7f666_row13_col1\" class=\"data row13 col1\" >reprocess</td>\n",
       "      <td id=\"T_7f666_row13_col2\" class=\"data row13 col2\" >reprocess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_7f666_row14_col0\" class=\"data row14 col0\" >('information', 'NN')</td>\n",
       "      <td id=\"T_7f666_row14_col1\" class=\"data row14 col1\" >information</td>\n",
       "      <td id=\"T_7f666_row14_col2\" class=\"data row14 col2\" >inform</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_7f666_row15_col0\" class=\"data row15 col0\" >('provided', 'VBN')</td>\n",
       "      <td id=\"T_7f666_row15_col1\" class=\"data row15 col1\" >provide</td>\n",
       "      <td id=\"T_7f666_row15_col2\" class=\"data row15 col2\" >provid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_7f666_row16_col0\" class=\"data row16 col0\" >('since', 'IN')</td>\n",
       "      <td id=\"T_7f666_row16_col1\" class=\"data row16 col1\" >since</td>\n",
       "      <td id=\"T_7f666_row16_col2\" class=\"data row16 col2\" >sinc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_7f666_row17_col0\" class=\"data row17 col0\" >('lot', 'NN')</td>\n",
       "      <td id=\"T_7f666_row17_col1\" class=\"data row17 col1\" >lot</td>\n",
       "      <td id=\"T_7f666_row17_col2\" class=\"data row17 col2\" >lot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_7f666_row18_col0\" class=\"data row18 col0\" >('number', 'NN')</td>\n",
       "      <td id=\"T_7f666_row18_col1\" class=\"data row18 col1\" >number</td>\n",
       "      <td id=\"T_7f666_row18_col2\" class=\"data row18 col2\" >number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_7f666_row19_col0\" class=\"data row19 col0\" >('provided', 'VBN')</td>\n",
       "      <td id=\"T_7f666_row19_col1\" class=\"data row19 col1\" >provide</td>\n",
       "      <td id=\"T_7f666_row19_col2\" class=\"data row19 col2\" >provid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_7f666_row20_col0\" class=\"data row20 col0\" >('information', 'NN')</td>\n",
       "      <td id=\"T_7f666_row20_col1\" class=\"data row20 col1\" >information</td>\n",
       "      <td id=\"T_7f666_row20_col2\" class=\"data row20 col2\" >inform</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_7f666_row21_col0\" class=\"data row21 col0\" >('cannot', 'NN')</td>\n",
       "      <td id=\"T_7f666_row21_col1\" class=\"data row21 col1\" >cannot</td>\n",
       "      <td id=\"T_7f666_row21_col2\" class=\"data row21 col2\" >cannot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f666_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_7f666_row22_col0\" class=\"data row22 col0\" >('determined', 'VBD')</td>\n",
       "      <td id=\"T_7f666_row22_col1\" class=\"data row22 col1\" >determine</td>\n",
       "      <td id=\"T_7f666_row22_col2\" class=\"data row22 col2\" >determin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x11f0627a0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "# Using itertuples(), the columns must be addressed using thier position.\n",
    "# Here's a map of position to name:\n",
    "# row[1]: ROW_ID\n",
    "# row[2]: FOI_TEXT\n",
    "# row[3]: DEVICE_PROBLEM_CODE\n",
    "# row[4]: DEVICE_PROBLEM_TEXT\n",
    "for row in df.itertuples():\n",
    "    for sentence in row[7].split(\".\"):\n",
    "        if sentence != \"\":\n",
    "            sentences.append([row[1], row[3], row[4], sentence])\n",
    "\n",
    "sentences_df = pd.DataFrame(\n",
    "    sentences,\n",
    "    columns=[\n",
    "        \"ROW_ID\",\n",
    "        \"DEVICE_PROBLEM_CODE\",\n",
    "        \"DEVICE_PROBLEM_TEXT\",\n",
    "        \"SENTENCIZED_FOI_TEXT\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "compare_lemma_stem_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "62a27c3d-d6de-47cc-bca1-32e3a716f168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95712, 4)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f4a1a46-8619-400b-84f6-c4ed8003017e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>DEVICE_PROBLEM_CODE</th>\n",
       "      <th>DEVICE_PROBLEM_TEXT</th>\n",
       "      <th>SENTENCIZED_FOI_TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>106741</td>\n",
       "      <td>106903842</td>\n",
       "      <td>N</td>\n",
       "      <td>BASED ON ADDITIONAL INFORMATION RECEIVED THIS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106741</td>\n",
       "      <td>106903842</td>\n",
       "      <td>N</td>\n",
       "      <td>IF INFORMATION IS PROVIDED IN THE FUTURE, A S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>106742</td>\n",
       "      <td>106903843</td>\n",
       "      <td>D</td>\n",
       "      <td>BASED ON ADDITIONAL INFORMATION RECEIVED THIS ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ROW_ID DEVICE_PROBLEM_CODE DEVICE_PROBLEM_TEXT  \\\n",
       "0  106741           106903842                   N   \n",
       "1  106741           106903842                   N   \n",
       "2  106742           106903843                   D   \n",
       "\n",
       "                                SENTENCIZED_FOI_TEXT  \n",
       "0  BASED ON ADDITIONAL INFORMATION RECEIVED THIS ...  \n",
       "1   IF INFORMATION IS PROVIDED IN THE FUTURE, A S...  \n",
       "2  BASED ON ADDITIONAL INFORMATION RECEIVED THIS ...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f6aa46d7-6b15-4539-89c3-acc16c1c6c2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BASED ON ADDITIONAL INFORMATION RECEIVED THIS COMPLAINT IS NOT A MEDTRONIC PRODUCT'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df[\"SENTENCIZED_FOI_TEXT\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cea1af87-cd7e-4400-bc31-fcc8f3fea5cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand Contractions, Tokenize, and Convert to Lowercase\n",
    "sentences_df[\"TOKENIZED_SENTENCES\"] = sentences_df[\"SENTENCIZED_FOI_TEXT\"].apply(\n",
    "    lambda x: [contractions.fix(word).lower() for word in x.split()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "25d9b7de-2e44-4ecd-b63a-8ade98fe7a3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['based',\n",
       " 'on',\n",
       " 'additional',\n",
       " 'information',\n",
       " 'received',\n",
       " 'this',\n",
       " 'complaint',\n",
       " 'is',\n",
       " 'not',\n",
       " 'a',\n",
       " 'medtronic',\n",
       " 'product']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df[\"TOKENIZED_SENTENCES\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "daad689d-e1e4-48ee-aeaa-ef244a0773a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['based',\n",
       " 'on',\n",
       " 'additional',\n",
       " 'information',\n",
       " 'received',\n",
       " 'this',\n",
       " 'complaint',\n",
       " 'is',\n",
       " 'not',\n",
       " 'a',\n",
       " 'medtronic',\n",
       " 'product']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "sentences_df[\"NOPUNCT_SENTENCES\"] = sentences_df[\"TOKENIZED_SENTENCES\"].apply(\n",
    "    lambda x: [remove_punctuation(word) for word in x]\n",
    ")\n",
    "sentences_df[\"NOPUNCT_SENTENCES\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d3f2f22a-6f55-4ea6-8382-e97993d5815c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['based',\n",
       " 'on',\n",
       " 'additional',\n",
       " 'information',\n",
       " 'received',\n",
       " 'this',\n",
       " 'complaint',\n",
       " 'is',\n",
       " 'not',\n",
       " 'a',\n",
       " 'medtronic',\n",
       " 'product']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stop words\n",
    "sentences_df[\"NOSTOPWORDS_SENTENCES\"] = sentences_df[\"NOPUNCT_SENTENCES\"].apply(\n",
    "    lambda x: remove_stopwords(x)\n",
    ")\n",
    "sentences_df[\"NOSTOPWORDS_SENTENCES\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e96ddcd3-6711-4a56-ad2c-cd4bf5e84988",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('based', 'VBN'),\n",
       " ('on', 'IN'),\n",
       " ('additional', 'JJ'),\n",
       " ('information', 'NN'),\n",
       " ('received', 'VBD'),\n",
       " ('this', 'DT'),\n",
       " ('complaint', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('not', 'RB'),\n",
       " ('a', 'DT'),\n",
       " ('medtronic', 'JJ'),\n",
       " ('product', 'NN')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply POS Tagging\n",
    "sentences_df[\"POS_SENTENCES\"] = sentences_df[\"NOSTOPWORDS_SENTENCES\"].apply(\n",
    "    nltk.pos_tag\n",
    ")\n",
    "sentences_df[\"POS_SENTENCES\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2f36e88a-a2c6-4e68-98d9-ad4d9d8339ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to join tokens that have been lemmatized and stemmed\n",
    "def join_tokenized_sentence(tokens):\n",
    "    joined_words = []\n",
    "\n",
    "    for word in tokens:\n",
    "        joined_words.append(word)\n",
    "\n",
    "    # Join the stemmed words back into a sentence\n",
    "    return \" \".join(joined_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880f8066-5d31-4abf-996d-269eef7fd804",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 10.A Lemmatize Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "538fbb13-a592-4502-abb6-8cb05deeccc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['base',\n",
       " 'on',\n",
       " 'additional',\n",
       " 'information',\n",
       " 'receive',\n",
       " 'this',\n",
       " 'complaint',\n",
       " 'be',\n",
       " 'not',\n",
       " 'a',\n",
       " 'medtronic',\n",
       " 'product']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df[\"TOKEN_LEMMATIZED_SENTENCES\"] = sentences_df[\"POS_SENTENCES\"].apply(\n",
    "    lemmatize_text\n",
    ")\n",
    "sentences_df[\"TOKEN_LEMMATIZED_SENTENCES\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "589301ef-dcce-434f-829b-23f4ca684dc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'base on additional information receive this complaint be not a medtronic product'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df[\"LEMMATIZED_SENTENCES\"] = sentences_df[\"TOKEN_LEMMATIZED_SENTENCES\"].apply(\n",
    "    join_tokenized_sentence\n",
    ")\n",
    "sentences_df[\"LEMMATIZED_SENTENCES\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa224d6e-72ca-4548-8091-82cdb05591d3",
   "metadata": {},
   "source": [
    "### 10.B Stem Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a8b31b8f-b03f-4db2-8f8f-ef2d89c4e450",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['base',\n",
       " 'on',\n",
       " 'addit',\n",
       " 'inform',\n",
       " 'receiv',\n",
       " 'thi',\n",
       " 'complaint',\n",
       " 'is',\n",
       " 'not',\n",
       " 'a',\n",
       " 'medtron',\n",
       " 'product']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a new column called 'STEMMED_SENTENCES'\n",
    "sentences_df[\"TOKEN_STEMMED_SENTENCES\"] = sentences_df[\"POS_SENTENCES\"].apply(\n",
    "    stem_words\n",
    ")\n",
    "sentences_df[\"TOKEN_STEMMED_SENTENCES\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "13731b1c-2b14-4295-8bc0-132c74d1bac4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'base on addit inform receiv thi complaint is not a medtron product'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df[\"STEMMED_SENTENCES\"] = sentences_df[\"TOKEN_STEMMED_SENTENCES\"].apply(\n",
    "    join_tokenized_sentence\n",
    ")\n",
    "sentences_df[\"STEMMED_SENTENCES\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74f580b-740d-4631-9816-37ce859dde28",
   "metadata": {},
   "source": [
    "## Review the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "66c4f4aa-b714-40d5-a097-1b1a4ea51c5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DF COLUMN NAMES</th>\n",
       "      <th>EXAMPLE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unnamed: 0</td>\n",
       "      <td>106741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MDR_REPORT_KEY</td>\n",
       "      <td>6383024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MDR_TEXT_KEY</td>\n",
       "      <td>106903842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEXT_TYPE_CODE</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PATIENT_SEQUENCE_NUMBER</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DATE_REPORT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FOI_TEXT</td>\n",
       "      <td>BASED ON ADDITIONAL INFORMATION RECEIVED THIS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DEVICE_EVENT_KEY</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>IMPLANT_FLAG</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DATE_REMOVED_FLAG</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>DEVICE_SEQUENCE_NO</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>DATE_RECEIVED</td>\n",
       "      <td>2017/03/06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>BRAND_NAME</td>\n",
       "      <td>UNKNOWN URETEX MESH PRODUCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GENERIC_NAME</td>\n",
       "      <td>MESH, SURGICAL, SYNTHETIC, UROGYNECOLOGIC, FOR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MANUFACTURER_D_NAME</td>\n",
       "      <td>SOFRADIM PRODUCTION SAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MANUFACTURER_D_ADDRESS_1</td>\n",
       "      <td>116 AVENUE DU FORMANS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MANUFACTURER_D_ADDRESS_2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MANUFACTURER_D_CITY</td>\n",
       "      <td>TREVOUX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MANUFACTURER_D_STATE_CODE</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MANUFACTURER_D_ZIP_CODE</td>\n",
       "      <td>01600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>MANUFACTURER_D_ZIP_CODE_EXT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>MANUFACTURER_D_COUNTRY_CODE</td>\n",
       "      <td>FR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>MANUFACTURER_D_POSTAL_CODE</td>\n",
       "      <td>01600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>DEVICE_OPERATOR</td>\n",
       "      <td>0HP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>EXPIRATION_DATE_OF_DEVICE</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>MODEL_NUMBER</td>\n",
       "      <td>UNKNOWN URETEX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>CATALOG_NUMBER</td>\n",
       "      <td>UNKNOWN URETEX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LOT_NUMBER</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>OTHER_ID_NUMBER</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>DEVICE_AVAILABILITY</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>DATE_RETURNED_TO_MANUFACTURER</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>DEVICE_REPORT_PRODUCT_CODE</td>\n",
       "      <td>OTN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>DEVICE_AGE_TEXT</td>\n",
       "      <td>DA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>DEVICE_EVALUATED_BY_MANUFACTUR</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>COMBINATION_PRODUCT_FLAG</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>UDI-DI</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>UDI-PUBLIC</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>TOKENIZED_TEXT</td>\n",
       "      <td>[based, on, additional, information, received,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>NOPUNCT_TEXT</td>\n",
       "      <td>[based, on, additional, information, received,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>NOSTOPWORDS_TEXT</td>\n",
       "      <td>[based, additional, information, received, com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>NODIGITS_TEXT</td>\n",
       "      <td>[based, additional, information, received, com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>POS_TEXT</td>\n",
       "      <td>[(based, VBN), (additional, JJ), (information,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>LEMMATIZED_TEXT</td>\n",
       "      <td>[base, additional, information, receive, compl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>STEMMED_TEXT</td>\n",
       "      <td>[base, addit, inform, receiv, complaint, medtr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   DF COLUMN NAMES  \\\n",
       "0                       Unnamed: 0   \n",
       "1                   MDR_REPORT_KEY   \n",
       "2                     MDR_TEXT_KEY   \n",
       "3                   TEXT_TYPE_CODE   \n",
       "4          PATIENT_SEQUENCE_NUMBER   \n",
       "5                      DATE_REPORT   \n",
       "6                         FOI_TEXT   \n",
       "7                 DEVICE_EVENT_KEY   \n",
       "8                     IMPLANT_FLAG   \n",
       "9                DATE_REMOVED_FLAG   \n",
       "10              DEVICE_SEQUENCE_NO   \n",
       "11                   DATE_RECEIVED   \n",
       "12                      BRAND_NAME   \n",
       "13                    GENERIC_NAME   \n",
       "14             MANUFACTURER_D_NAME   \n",
       "15        MANUFACTURER_D_ADDRESS_1   \n",
       "16        MANUFACTURER_D_ADDRESS_2   \n",
       "17             MANUFACTURER_D_CITY   \n",
       "18       MANUFACTURER_D_STATE_CODE   \n",
       "19         MANUFACTURER_D_ZIP_CODE   \n",
       "20     MANUFACTURER_D_ZIP_CODE_EXT   \n",
       "21     MANUFACTURER_D_COUNTRY_CODE   \n",
       "22      MANUFACTURER_D_POSTAL_CODE   \n",
       "23                 DEVICE_OPERATOR   \n",
       "24       EXPIRATION_DATE_OF_DEVICE   \n",
       "25                    MODEL_NUMBER   \n",
       "26                  CATALOG_NUMBER   \n",
       "27                      LOT_NUMBER   \n",
       "28                 OTHER_ID_NUMBER   \n",
       "29             DEVICE_AVAILABILITY   \n",
       "30   DATE_RETURNED_TO_MANUFACTURER   \n",
       "31      DEVICE_REPORT_PRODUCT_CODE   \n",
       "32                 DEVICE_AGE_TEXT   \n",
       "33  DEVICE_EVALUATED_BY_MANUFACTUR   \n",
       "34        COMBINATION_PRODUCT_FLAG   \n",
       "35                          UDI-DI   \n",
       "36                      UDI-PUBLIC   \n",
       "37                  TOKENIZED_TEXT   \n",
       "38                    NOPUNCT_TEXT   \n",
       "39                NOSTOPWORDS_TEXT   \n",
       "40                   NODIGITS_TEXT   \n",
       "41                        POS_TEXT   \n",
       "42                 LEMMATIZED_TEXT   \n",
       "43                    STEMMED_TEXT   \n",
       "\n",
       "                                              EXAMPLE  \n",
       "0                                              106741  \n",
       "1                                             6383024  \n",
       "2                                           106903842  \n",
       "3                                                   N  \n",
       "4                                                   1  \n",
       "5                                                      \n",
       "6   BASED ON ADDITIONAL INFORMATION RECEIVED THIS ...  \n",
       "7                                                      \n",
       "8                                                      \n",
       "9                                                      \n",
       "10                                                  1  \n",
       "11                                         2017/03/06  \n",
       "12                        UNKNOWN URETEX MESH PRODUCT  \n",
       "13  MESH, SURGICAL, SYNTHETIC, UROGYNECOLOGIC, FOR...  \n",
       "14                            SOFRADIM PRODUCTION SAS  \n",
       "15                              116 AVENUE DU FORMANS  \n",
       "16                                                     \n",
       "17                                            TREVOUX  \n",
       "18                                                     \n",
       "19                                              01600  \n",
       "20                                                     \n",
       "21                                                 FR  \n",
       "22                                              01600  \n",
       "23                                                0HP  \n",
       "24                                                     \n",
       "25                                     UNKNOWN URETEX  \n",
       "26                                     UNKNOWN URETEX  \n",
       "27                                                     \n",
       "28                                                     \n",
       "29                                                  N  \n",
       "30                                                     \n",
       "31                                                OTN  \n",
       "32                                                 DA  \n",
       "33                                                  N  \n",
       "34                                                  N  \n",
       "35                                                     \n",
       "36                                                     \n",
       "37  [based, on, additional, information, received,...  \n",
       "38  [based, on, additional, information, received,...  \n",
       "39  [based, additional, information, received, com...  \n",
       "40  [based, additional, information, received, com...  \n",
       "41  [(based, VBN), (additional, JJ), (information,...  \n",
       "42  [base, additional, information, receive, compl...  \n",
       "43  [base, addit, inform, receiv, complaint, medtr...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new dataframe with just one row containing the column names\n",
    "column_names_df = pd.DataFrame(\n",
    "    {\n",
    "        \"DF COLUMN NAMES\": df.columns,\n",
    "    }\n",
    ")\n",
    "\n",
    "example = []\n",
    "\n",
    "for col in df.columns:\n",
    "    example.append(df[col][0])\n",
    "\n",
    "column_names_df[\"EXAMPLE\"] = example\n",
    "column_names_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "95aba4e3-0454-46c6-a73a-f4f644d567d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SENTENCES DF COLUMN NAMES</th>\n",
       "      <th>EXAMPLE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ROW_ID</td>\n",
       "      <td>106741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DEVICE_PROBLEM_CODE</td>\n",
       "      <td>106903842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DEVICE_PROBLEM_TEXT</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SENTENCIZED_FOI_TEXT</td>\n",
       "      <td>BASED ON ADDITIONAL INFORMATION RECEIVED THIS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TOKENIZED_SENTENCES</td>\n",
       "      <td>[based, on, additional, information, received,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NOPUNCT_SENTENCES</td>\n",
       "      <td>[based, on, additional, information, received,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NOSTOPWORDS_SENTENCES</td>\n",
       "      <td>[based, on, additional, information, received,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>POS_SENTENCES</td>\n",
       "      <td>[(based, VBN), (on, IN), (additional, JJ), (in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TOKEN_LEMMATIZED_SENTENCES</td>\n",
       "      <td>[base, on, additional, information, receive, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LEMMATIZED_SENTENCES</td>\n",
       "      <td>base on additional information receive this co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TOKEN_STEMMED_SENTENCES</td>\n",
       "      <td>[base, on, addit, inform, receiv, thi, complai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>STEMMED_SENTENCES</td>\n",
       "      <td>base on addit inform receiv thi complaint is n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     SENTENCES DF COLUMN NAMES  \\\n",
       "0                       ROW_ID   \n",
       "1          DEVICE_PROBLEM_CODE   \n",
       "2          DEVICE_PROBLEM_TEXT   \n",
       "3         SENTENCIZED_FOI_TEXT   \n",
       "4          TOKENIZED_SENTENCES   \n",
       "5            NOPUNCT_SENTENCES   \n",
       "6        NOSTOPWORDS_SENTENCES   \n",
       "7                POS_SENTENCES   \n",
       "8   TOKEN_LEMMATIZED_SENTENCES   \n",
       "9         LEMMATIZED_SENTENCES   \n",
       "10     TOKEN_STEMMED_SENTENCES   \n",
       "11           STEMMED_SENTENCES   \n",
       "\n",
       "                                              EXAMPLE  \n",
       "0                                              106741  \n",
       "1                                           106903842  \n",
       "2                                                   N  \n",
       "3   BASED ON ADDITIONAL INFORMATION RECEIVED THIS ...  \n",
       "4   [based, on, additional, information, received,...  \n",
       "5   [based, on, additional, information, received,...  \n",
       "6   [based, on, additional, information, received,...  \n",
       "7   [(based, VBN), (on, IN), (additional, JJ), (in...  \n",
       "8   [base, on, additional, information, receive, t...  \n",
       "9   base on additional information receive this co...  \n",
       "10  [base, on, addit, inform, receiv, thi, complai...  \n",
       "11  base on addit inform receiv thi complaint is n...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new dataframe with just one row containing the column names\n",
    "column_names_df = pd.DataFrame(\n",
    "    {\n",
    "        \"SENTENCES DF COLUMN NAMES\": sentences_df.columns,\n",
    "    }\n",
    ")\n",
    "\n",
    "example = []\n",
    "\n",
    "for col in sentences_df.columns:\n",
    "    example.append(sentences_df[col][0])\n",
    "\n",
    "column_names_df[\"EXAMPLE\"] = example\n",
    "column_names_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13143c09-f9cd-4c01-b7b7-5e3994aa21e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save the preproecssed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "98e853ae-ef27-4d92-a71a-b09efd9f101b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv(f\"./data/preprocessed_data.csv\", index=False)\n",
    "\n",
    "bow_df.to_csv(f\"./data/bag_of_words_data.csv\", index=False)\n",
    "\n",
    "tfidf_df.to_csv(f\"./data/tfidf_data.csv\", index=False)\n",
    "\n",
    "sentences_df.to_csv(f\"./data/sentences_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc2654d-14e8-4709-8289-9922127f9f88",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Upload All Output to an S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9b03bb42-336b-406d-84d2-dbda67a93f9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: 21-Preprocess-Combined-Data-v2/preprocessed_data.csv to s3://praxis-2023-html-output/preprocessed_data.csv\n",
      "upload: 21-Preprocess-Combined-Data-v2/dataframe.pickle to s3://praxis-2023-html-output/dataframe.pickle\n",
      "upload: 21-Preprocess-Combined-Data-v2/sentences_data.csv to s3://praxis-2023-html-output/sentences_data.csv\n",
      "upload: 21-Preprocess-Combined-Data-v2/tfidf_data.csv to s3://praxis-2023-html-output/tfidf_data.csv\n",
      "upload: 21-Preprocess-Combined-Data-v2/bag_of_words_data.csv to s3://praxis-2023-html-output/bag_of_words_data.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Create the upload command using the AWS command line interface\n",
    "command = [\n",
    "    \"aws\",\n",
    "    \"s3\",\n",
    "    \"sync\",\n",
    "    working_directory,\n",
    "    f\"s3://praxis-2023-html-output/\",\n",
    "    \"--exclude\",\n",
    "    f\"*/.ipynb_checkpoints/*\",\n",
    "    \"--no-progress\",\n",
    "]\n",
    "\n",
    "# Run the command and wait for it to complete\n",
    "output = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "# Print the command's output\n",
    "print(output.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd1d2b88-dc2a-4829-acdb-d270073299d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Assume `df` is the dataframe you want to save\n",
    "with open(f\"{working_directory}/dataframe.pickle\", \"wb\") as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cfe24156-2f48-4ede-bb5f-1b0a19e59ae2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fin\n"
     ]
    }
   ],
   "source": [
    "print(\"fin\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
