{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/stress_urinary_incontinence.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    # Remove special characters, digits, and extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text.lower()  # Convert to lowercase\n",
    "\n",
    "# Apply the cleaning function to the text column\n",
    "df['cleaned_text'] = df['FOI_TEXT'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/zhaohengchuan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to segment text into sentences\n",
    "def segment_text(text):\n",
    "    return nltk.sent_tokenize(text)\n",
    "\n",
    "# Apply the function to segment the cleaned text\n",
    "df['segmented_text'] = df['cleaned_text'].apply(segment_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it was reported to boston scientific corporation that an obtryx system was used during a transobturator tape for stress incontinence procedure performed on b6 2007 according to the complainant on b6 2014 the mesh was found to have eroded until it lay across the urethral lumen and a large bladder stone formed around the mesh the patient underwent transurethral surgery to endoscopically to crush up and remove stones and all visible mesh from within the urethral lumen she was catheterized for one week after the surgery and reportedly her symptoms settled down well all other information is unknown should additional relevant details become available a supplemental report will be submitted']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['segmented_text'][500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Generating Questions: 100%|██████████| 20/20 [00:14<00:00,  1.38it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "# Load T5 model and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('valhalla/t5-small-qg-hl')\n",
    "model = T5ForConditionalGeneration.from_pretrained('valhalla/t5-small-qg-hl')\n",
    "\n",
    "def generate_questions(text):\n",
    "    # Prepare the input for the model\n",
    "    input_text = f\"generate question: {text}\"\n",
    "    inputs = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    \n",
    "    # Generate questions\n",
    "    outputs = model.generate(inputs, max_length=150, num_beams=5, early_stopping=True)\n",
    "    question = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return question\n",
    "\n",
    "# Example of generating questions for each sentence with progress bar\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating Questions\"):\n",
    "    questions = []\n",
    "    for sentence in row['segmented_text']:\n",
    "        question = generate_questions(sentence)\n",
    "        questions.append(question)\n",
    "    df.at[index, 'questions'] = questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load T5 model and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('valhalla/t5-small-qg-hl')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = T5ForConditionalGeneration.from_pretrained('valhalla/t5-small-qg-hl')\n",
    "\n",
    "def generate_questions(text):\n",
    "    # Prepare the input for the model\n",
    "    input_text = f\"generate question: {text}\"\n",
    "    inputs = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    \n",
    "    # Generate questions\n",
    "    outputs = model.generate(inputs, max_length=150, num_beams=5, early_stopping=True)\n",
    "    question = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return question\n",
    "\n",
    "# Example of generating questions for each sentence\n",
    "for index, row in df.iterrows():\n",
    "    questions = []\n",
    "    for sentence in row['segmented_text']:\n",
    "        question = generate_questions(sentence)\n",
    "        questions.append(question)\n",
    "    df.at[index, 'questions'] = questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            question  \\\n",
      "0                 <pad> What is a medtronic product?   \n",
      "1           <pad> What is the name of the complaint?   \n",
      "2  <pad> What will a supplemental report be issue...   \n",
      "3  <pad> What was the manufacturer reference numb...   \n",
      "4  <pad> What was the preoperative and postoperat...   \n",
      "\n",
      "                                      answer  \\\n",
      "0  THIS COMPLAINT IS NOT A MEDTRONIC PRODUCT   \n",
      "1                                  MEDTRONIC   \n",
      "2      A SUPPLEMENTAL REPORT WILL BE ISSUED.   \n",
      "3                                     (B)(4)   \n",
      "4                                    EROSION   \n",
      "\n",
      "                                             context  \n",
      "0  BASED ON ADDITIONAL INFORMATION RECEIVED THIS ...  \n",
      "1  BASED ON ADDITIONAL INFORMATION RECEIVED THIS ...  \n",
      "2  IF INFORMATION IS PROVIDED IN THE FUTURE, A SU...  \n",
      "3  MANUFACTURER REFERENCE NUMBER: (B)(4). INCIDEN...  \n",
      "4  THE PATIENT'S ATTORNEY ALLEGED A DEFICIENCY AG...  \n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained QA model\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
    "\n",
    "def generate_answer(question, context):\n",
    "    # Extract the answer from the context based on the question\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    return result['answer']\n",
    "\n",
    "# Generate answers based on the generated questions\n",
    "qa_pairs = []\n",
    "for index, row in df.iterrows():\n",
    "    context = row['FOI_TEXT']\n",
    "    for question in row['questions']:\n",
    "        answer = generate_answer(question, context)\n",
    "        qa_pairs.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"context\": context\n",
    "        })\n",
    "\n",
    "# Convert QA pairs to DataFrame\n",
    "qa_df = pd.DataFrame(qa_pairs)\n",
    "print(qa_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;pad&gt; What is a medtronic product?</td>\n",
       "      <td>THIS COMPLAINT IS NOT A MEDTRONIC PRODUCT</td>\n",
       "      <td>BASED ON ADDITIONAL INFORMATION RECEIVED THIS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;pad&gt; What is the name of the complaint?</td>\n",
       "      <td>MEDTRONIC</td>\n",
       "      <td>BASED ON ADDITIONAL INFORMATION RECEIVED THIS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;pad&gt; What will a supplemental report be issue...</td>\n",
       "      <td>A SUPPLEMENTAL REPORT WILL BE ISSUED.</td>\n",
       "      <td>IF INFORMATION IS PROVIDED IN THE FUTURE, A SU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;pad&gt; What was the manufacturer reference numb...</td>\n",
       "      <td>(B)(4)</td>\n",
       "      <td>MANUFACTURER REFERENCE NUMBER: (B)(4). INCIDEN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;pad&gt; What was the preoperative and postoperat...</td>\n",
       "      <td>EROSION</td>\n",
       "      <td>THE PATIENT'S ATTORNEY ALLEGED A DEFICIENCY AG...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                 <pad> What is a medtronic product?   \n",
       "1           <pad> What is the name of the complaint?   \n",
       "2  <pad> What will a supplemental report be issue...   \n",
       "3  <pad> What was the manufacturer reference numb...   \n",
       "4  <pad> What was the preoperative and postoperat...   \n",
       "\n",
       "                                      answer  \\\n",
       "0  THIS COMPLAINT IS NOT A MEDTRONIC PRODUCT   \n",
       "1                                  MEDTRONIC   \n",
       "2      A SUPPLEMENTAL REPORT WILL BE ISSUED.   \n",
       "3                                     (B)(4)   \n",
       "4                                    EROSION   \n",
       "\n",
       "                                             context  \n",
       "0  BASED ON ADDITIONAL INFORMATION RECEIVED THIS ...  \n",
       "1  BASED ON ADDITIONAL INFORMATION RECEIVED THIS ...  \n",
       "2  IF INFORMATION IS PROVIDED IN THE FUTURE, A SU...  \n",
       "3  MANUFACTURER REFERENCE NUMBER: (B)(4). INCIDEN...  \n",
       "4  THE PATIENT'S ATTORNEY ALLEGED A DEFICIENCY AG...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained QA model\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
    "\n",
    "def generate_answer(question, context):\n",
    "    # Extract the answer from the context based on the question\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    return result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PREOPERATIVE AND POSTOPERATIVE DIAGNOSIS'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(\"What is stress urinary incontinence\",\"THE PATIENT'S ATTORNEY ALLEGED A DEFICIENCY AGAINST THE DEVICE RESULTING IN AN UNSPECIFIED ADVERSE OUTCOME. PRODUCT WAS USED FOR THERAPEUTIC TREATMENT. THE PREOPERATIVE AND POSTOPERATIVE DIAGNOSIS WAS PELVIC PAIN, MENOMETRORRHAGIA, UTERINE FIBROID, STRESS URINARY INCONTINENCE, AND HYPERMOBILE URETHRA. THE PROCEDURE PERFORMED WAS A TRANS-OBTURATOR TAPE AND LAPAROSCOPIC SUPRA-CERVICAL HYSTERECTOMY. IN (B)(6) 2007 THE PATIENT UNDERWENT AN ADDITIONAL PROCEDURE FOR DYSMENORRHEA, PELVIC PAIN, AND PMS PELVIC CRAMPING STATUS POST SUPRACERVICAL LAPAROSCOPY HYSTERECTOMY. THE PROCEDURE PERFORMED WAS AN OPERATIVE LAPAROSCOPY WITH ADHESIOLYSIS, BILATERAL SALPINGO-OOPHORECTOMY AND TRACHELECTOMY.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load your QA pairs\n",
    "# qa_df = pd.read_csv('generated_qa_pairs.csv')\n",
    "\n",
    "# Prepare dataset for fine-tuning\n",
    "data = []\n",
    "for index, row in qa_df.iterrows():\n",
    "    data.append({\"input\": row['question'], \"output\": row['answer']})\n",
    "\n",
    "# Create a Hugging Face Dataset\n",
    "dataset = Dataset.from_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1776412faab45d380448d066730749a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the input and output\n",
    "    input_ids = tokenizer(examples['input'], truncation=True, padding='max_length', max_length=50)\n",
    "    output_ids = tokenizer(examples['output'], truncation=True, padding='max_length', max_length=50)\n",
    "    \n",
    "    # Prepare labels (shifted input)\n",
    "    labels = output_ids['input_ids']\n",
    "    \n",
    "    # Return the tokenized inputs and labels\n",
    "    return {\n",
    "        'input_ids': input_ids['input_ids'],\n",
    "        'attention_mask': input_ids['attention_mask'],\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee03550e358492c99c38d439cab1c96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 32.8177, 'train_samples_per_second': 0.609, 'train_steps_per_second': 0.305, 'train_loss': 4.321907424926758, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_gpt2/tokenizer_config.json',\n",
       " './fine_tuned_gpt2/special_tokens_map.json',\n",
       " './fine_tuned_gpt2/vocab.json',\n",
       " './fine_tuned_gpt2/merges.txt',\n",
       " './fine_tuned_gpt2/added_tokens.json')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('./fine_tuned_gpt2')\n",
    "tokenizer.save_pretrained('./fine_tuned_gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_answer(question, max_length=50):\n",
    "    # Tokenize the input question\n",
    "    inputs = tokenizer.encode(question, return_tensors='pt')  # Add batch dimension\n",
    "\n",
    "    # Generate the response\n",
    "    with torch.no_grad():  # Disable gradient calculations\n",
    "        outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1)\n",
    "\n",
    "    # Decode the generated tokens\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_answer(question, context, max_length=50):\n",
    "    # Prepare the input text with question and context\n",
    "    input_text = f\"question: {question} context: {context}\"\n",
    "    \n",
    "    # Tokenize the input (make sure pad_token is set)\n",
    "    inputs = tokenizer.encode(input_text, return_tensors='pt', truncation=True)\n",
    "    \n",
    "    # Ensure the pad token is correctly set\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Generate the response\n",
    "    with torch.no_grad():  # Disable gradient calculations for inference\n",
    "        outputs = model.generate(\n",
    "            inputs, \n",
    "            max_length=max_length, \n",
    "            num_return_sequences=1, \n",
    "            pad_token_id=tokenizer.eos_token_id  # Ensure padding is handled properly\n",
    "        )\n",
    "\n",
    "    # Decode the generated tokens\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: what is sui?\n",
      "Answer: question: what is sui? context: In generat, sui is a disease that is caused by the use of a certain type of food.\n"
     ]
    }
   ],
   "source": [
    "# Example question\n",
    "question = \"what is sui?\"\n",
    "\n",
    "# Generate an answer\n",
    "answer = generate_answer(question, \"In generat, sui is a disease\")\n",
    "\n",
    "# Print the answer\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: LAPAROSCOPIC SUPRA-CERVICAL HYSTERECTOMY\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained QA model\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
    "\n",
    "def generate_answer(question, context):\n",
    "    # Use the pipeline to generate an answer based on the question and context\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    return result['answer']\n",
    "\n",
    "# Example usage\n",
    "context = \"THE PATIENT'S ATTORNEY ALLEGED A DEFICIENCY AGAINST THE DEVICE RESULTING IN AN UNSPECIFIED ADVERSE OUTCOME. PRODUCT WAS USED FOR THERAPEUTIC TREATMENT. THE PREOPERATIVE AND POSTOPERATIVE DIAGNOSIS WAS PELVIC PAIN, MENOMETRORRHAGIA, UTERINE FIBROID, STRESS URINARY INCONTINENCE, AND HYPERMOBILE URETHRA. THE PROCEDURE PERFORMED WAS A TRANS-OBTURATOR TAPE AND LAPAROSCOPIC SUPRA-CERVICAL HYSTERECTOMY. IN (B)(6) 2007 THE PATIENT UNDERWENT AN ADDITIONAL PROCEDURE FOR DYSMENORRHEA, PELVIC PAIN, AND PMS PELVIC CRAMPING STATUS POST SUPRACERVICAL LAPAROSCOPY HYSTERECTOMY. THE PROCEDURE PERFORMED WAS AN OPERATIVE LAPAROSCOPY WITH ADHESIOLYSIS, BILATERAL SALPINGO-OOPHORECTOMY AND TRACHELECTOMY.\"\n",
    "question = \"What surgeries did the patient undergo??\"\n",
    "answer = generate_answer(question, context)\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/spacy/language.py:2195: FutureWarning: Possible set union at position 6328\n",
      "  deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load a pre-trained NER model for medical text\n",
    "nlp = spacy.load(\"en_core_sci_md\")  # SciSpacy model\n",
    "\n",
    "# Function to extract entities\n",
    "def extract_context(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([ent.text for ent in doc.ents])  # Join entities as potential context\n",
    "\n",
    "# Apply the function to extract context\n",
    "df['context'] = df['cleaned_text'].apply(extract_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load pre-trained T5 model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"ramsrigouthamg/t5_squad_v1\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"ramsrigouthamg/t5_squad_v1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate question based on context\n",
    "def generate_question(context):\n",
    "    input_text = \"generate question: \" + context\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_ids)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.notebook import tqdm\n",
    "# # Apply the function to generate questions\n",
    "# df['generated_question'] = df['context'].apply(generate_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "# Get total number of rows for progress calculation\n",
    "total_rows = len(df)\n",
    "\n",
    "# Apply the function with a progress bar\n",
    "generated_questions = []\n",
    "for index, row in tqdm(df.iterrows(), total=total_rows, desc=\"Processing rows\", unit=\"row\"):\n",
    "    generated_question = generate_question(row['context'])\n",
    "    generated_questions.append(generated_question)\n",
    "\n",
    "# Assign generated questions back to the DataFrame\n",
    "df['generated_question'] = generated_questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    qa_pairs.append({\n",
    "        \"question\": row['generated_question'],\n",
    "        \"context\": row['context']\n",
    "    })\n",
    "\n",
    "# Example output\n",
    "print(qa_pairs[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_data, val_data = train_test_split(qa_pairs, test_size=0.2)\n",
    "\n",
    "# Format the data for Hugging Face fine-tuning\n",
    "train_df = pd.DataFrame(train_data)\n",
    "val_df = pd.DataFrame(val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForQuestionAnswering, BertTokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load BERT model and tokenizer\n",
    "model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the QA data\n",
    "def tokenize_qa(examples):\n",
    "    inputs = tokenizer(examples['question'], examples['context'], truncation=True, padding=True)\n",
    "    return inputs\n",
    "\n",
    "# Convert train and validation data to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_qa, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_qa, batched=True)\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;pad&gt; What is a medtronic product?</td>\n",
       "      <td>THIS COMPLAINT IS NOT A MEDTRONIC PRODUCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;pad&gt; What is the name of the complaint?</td>\n",
       "      <td>MEDTRONIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;pad&gt; What will a supplemental report be issue...</td>\n",
       "      <td>A SUPPLEMENTAL REPORT WILL BE ISSUED.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;pad&gt; What was the manufacturer reference numb...</td>\n",
       "      <td>(B)(4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;pad&gt; What was the preoperative and postoperat...</td>\n",
       "      <td>EROSION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                 <pad> What is a medtronic product?   \n",
       "1           <pad> What is the name of the complaint?   \n",
       "2  <pad> What will a supplemental report be issue...   \n",
       "3  <pad> What was the manufacturer reference numb...   \n",
       "4  <pad> What was the preoperative and postoperat...   \n",
       "\n",
       "                                      answer  \n",
       "0  THIS COMPLAINT IS NOT A MEDTRONIC PRODUCT  \n",
       "1                                  MEDTRONIC  \n",
       "2      A SUPPLEMENTAL REPORT WILL BE ISSUED.  \n",
       "3                                     (B)(4)  \n",
       "4                                    EROSION  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = qa_df['question'].tolist()\n",
    "answers = qa_df['answer'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05757ed202164cd79d452f152a11c634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5534a84498e24dd682f77416318ce2bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a806f75e3a2478292ac4472131e564d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77bb9cf87ca746cebe5cd128db017e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bcb830fe95640c88cf618551b1c4802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "\n",
    "# Load the pre-trained BERT tokenizer and model for QA\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(question, answer, tokenizer):\n",
    "    # Tokenize the inputs for BERT\n",
    "    inputs = tokenizer.encode_plus(question, answer, return_tensors='pt')\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# Create a dataset object from pandas DataFrame\n",
    "dataset = Dataset.from_pandas(qa_df[['question', 'answer']])\n",
    "\n",
    "# Tokenize dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['question'], examples['answer'], truncation=True, padding='max_length')\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    evaluation_strategy=\"steps\",     # evaluate during training\n",
    "    per_device_train_batch_size=16,  # batch size for training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    num_train_epochs=3,              # number of training epochs\n",
    "    save_steps=10_000,               # save checkpoint every 10k steps\n",
    "    save_total_limit=2,              # limit the total amount of checkpoints\n",
    ")\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,  # your tokenized training dataset\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;pad&gt; What is a medtronic product?</td>\n",
       "      <td>THIS COMPLAINT IS NOT A MEDTRONIC PRODUCT</td>\n",
       "      <td>BASED ON ADDITIONAL INFORMATION RECEIVED THIS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;pad&gt; What is the name of the complaint?</td>\n",
       "      <td>MEDTRONIC</td>\n",
       "      <td>BASED ON ADDITIONAL INFORMATION RECEIVED THIS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;pad&gt; What will a supplemental report be issue...</td>\n",
       "      <td>A SUPPLEMENTAL REPORT WILL BE ISSUED.</td>\n",
       "      <td>IF INFORMATION IS PROVIDED IN THE FUTURE, A SU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;pad&gt; What was the manufacturer reference numb...</td>\n",
       "      <td>(B)(4)</td>\n",
       "      <td>MANUFACTURER REFERENCE NUMBER: (B)(4). INCIDEN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;pad&gt; What was the preoperative and postoperat...</td>\n",
       "      <td>EROSION</td>\n",
       "      <td>THE PATIENT'S ATTORNEY ALLEGED A DEFICIENCY AG...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                 <pad> What is a medtronic product?   \n",
       "1           <pad> What is the name of the complaint?   \n",
       "2  <pad> What will a supplemental report be issue...   \n",
       "3  <pad> What was the manufacturer reference numb...   \n",
       "4  <pad> What was the preoperative and postoperat...   \n",
       "\n",
       "                                      answer  \\\n",
       "0  THIS COMPLAINT IS NOT A MEDTRONIC PRODUCT   \n",
       "1                                  MEDTRONIC   \n",
       "2      A SUPPLEMENTAL REPORT WILL BE ISSUED.   \n",
       "3                                     (B)(4)   \n",
       "4                                    EROSION   \n",
       "\n",
       "                                             context  \n",
       "0  BASED ON ADDITIONAL INFORMATION RECEIVED THIS ...  \n",
       "1  BASED ON ADDITIONAL INFORMATION RECEIVED THIS ...  \n",
       "2  IF INFORMATION IS PROVIDED IN THE FUTURE, A SU...  \n",
       "3  MANUFACTURER REFERENCE NUMBER: (B)(4). INCIDEN...  \n",
       "4  THE PATIENT'S ATTORNEY ALLEGED A DEFICIENCY AG...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# Load your dataset (replace 'your_dataset.csv' with your actual file)\n",
    "# data = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Example of how the dataset should look\n",
    "# data = pd.DataFrame({\n",
    "#     'question': ['What is SUI?', 'What are the symptoms of SUI?'],\n",
    "#     'context': ['SUI stands for Stress Urinary Incontinence.', 'Symptoms include involuntary urine leakage.'],\n",
    "#     'answer': ['Stress Urinary Incontinence', 'involuntary urine leakage']\n",
    "# })\n",
    "\n",
    "def prepare_dataset(data):\n",
    "    dataset = []\n",
    "    for index, row in data.iterrows():\n",
    "        question = row['question']\n",
    "        context = row['context']\n",
    "        answer = row['answer']\n",
    "        \n",
    "        dataset.append({\n",
    "            'input_text': f\"question: {question} context: {context}\",\n",
    "            'target_text': answer\n",
    "        })\n",
    "    return pd.DataFrame(dataset)\n",
    "\n",
    "# Prepare the dataset\n",
    "prepared_data = prepare_dataset(qa_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6922915e942f433482c6498ae10199c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89fdbec80f5542c99e673a6bd65e5b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f8b53560aca4927bbc095270c82cd53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7443bc17dd1d4b7b9f3628c71a791947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411b4467eb004a6791ab20751d4df337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2841d23e22141a49976dcf03b827a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e3b2dde3b8a40039f5f395944c3ca45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4144: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Convert the prepared DataFrame to a Hugging Face Dataset\n",
    "huggingface_dataset = Dataset.from_pandas(prepared_data)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples['input_text'],\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    # Setup the labels\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples['target_text'],\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding='max_length'\n",
    "        )\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = huggingface_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and evaluation sets\n",
    "train_data, eval_data = train_test_split(prepared_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the split data into Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "eval_dataset = Dataset.from_pandas(eval_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc71a8ae7f43491598d9e6d364c8f51d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5211614cd29b4f1e8cf7a0b0abeacbc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenization function (reuse this for both datasets)\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples['input_text'],\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    # Setup the labels\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples['target_text'],\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding='max_length'\n",
    "        )\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize the training and evaluation datasets\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14af5a7acc814d78a11c7662c2f73e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1532795589c340a0ac15ada5e241472a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.824167251586914, 'eval_runtime': 1.7678, 'eval_samples_per_second': 2.263, 'eval_steps_per_second': 0.566, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf80990128e74ca28639619bf590f4fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.669038712978363, 'eval_runtime': 1.8499, 'eval_samples_per_second': 2.162, 'eval_steps_per_second': 0.541, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f9be72de7a4024bcc4641f0c6808dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4058331549167633, 'eval_runtime': 1.8033, 'eval_samples_per_second': 2.218, 'eval_steps_per_second': 0.555, 'epoch': 3.0}\n",
      "{'train_runtime': 99.4063, 'train_samples_per_second': 0.483, 'train_steps_per_second': 0.06, 'train_loss': 5.350741068522136, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=5.350741068522136, metrics={'train_runtime': 99.4063, 'train_samples_per_second': 0.483, 'train_steps_per_second': 0.06, 'total_flos': 6496406470656.0, 'train_loss': 5.350741068522136, 'epoch': 3.0})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=10_000,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,  # Add eval_dataset here\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is SUI?\n",
      "Answer: Stress Urinary Incontinence\n"
     ]
    }
   ],
   "source": [
    "# Example function to answer questions\n",
    "def answer_question(question, context):\n",
    "    input_text = f\"question: {question} context: {context}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    \n",
    "    output_ids = model.generate(input_ids)\n",
    "    answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Example usage\n",
    "test_question = \"What is SUI?\"\n",
    "test_context = \"SUI stands for Stress Urinary Incontinence.\"\n",
    "answer = answer_question(test_question, test_context)\n",
    "print(f\"Question: {test_question}\\nAnswer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What therapeutic treatment was the product used for?\n",
      "Answer: THERAPEUTIC TREATMENT\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "test_question = \"What therapeutic treatment was the product used for?\"\n",
    "test_context = \"\"\" \n",
    "THE PATIENT'S ATTORNEY ALLEGED A DEFICIENCY AGAINST THE DEVICE RESULTING IN AN UNSPECIFIED ADVERSE OUTCOME. PRODUCT WAS USED FOR THERAPEUTIC TREATMENT. THE PREOPERATIVE AND POSTOPERATIVE DIAGNOSIS WAS PELVIC PAIN, MENOMETRORRHAGIA, UTERINE FIBROID, STRESS URINARY INCONTINENCE, AND HYPERMOBILE URETHRA. THE PROCEDURE PERFORMED WAS A TRANS-OBTURATOR TAPE AND LAPAROSCOPIC SUPRA-CERVICAL HYSTERECTOMY. IN (B)(6) 2007 THE PATIENT UNDERWENT AN ADDITIONAL PROCEDURE FOR DYSMENORRHEA, PELVIC PAIN, AND PMS PELVIC CRAMPING STATUS POST SUPRACERVICAL LAPAROSCOPY HYSTERECTOMY. THE PROCEDURE PERFORMED WAS AN OPERATIVE LAPAROSCOPY WITH ADHESIOLYSIS, BILATERAL SALPINGO-OOPHORECTOMY AND TRACHELECTOMY.\n",
    "\"\"\"\n",
    "answer = answer_question(test_question, test_context)\n",
    "print(f\"Question: {test_question}\\nAnswer: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
