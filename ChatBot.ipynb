{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/stress_urinary_incontinence.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    # Remove special characters, digits, and extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text.lower()  # Convert to lowercase\n",
    "\n",
    "# Apply the cleaning function to the text column\n",
    "df['cleaned_text'] = df['FOI_TEXT'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/zhaohengchuan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to segment text into sentences\n",
    "def segment_text(text):\n",
    "    return nltk.sent_tokenize(text)\n",
    "\n",
    "# Apply the function to segment the cleaned text\n",
    "df['segmented_text'] = df['cleaned_text'].apply(segment_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/spacy/language.py:2195: FutureWarning: Possible set union at position 6328\n",
      "  deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load a pre-trained NER model for medical text\n",
    "nlp = spacy.load(\"en_core_sci_md\")  # SciSpacy model\n",
    "\n",
    "# Function to extract entities\n",
    "def extract_context(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([ent.text for ent in doc.ents])  # Join entities as potential context\n",
    "\n",
    "# Apply the function to extract context\n",
    "df['context'] = df['cleaned_text'].apply(extract_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load pre-trained T5 model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"ramsrigouthamg/t5_squad_v1\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"ramsrigouthamg/t5_squad_v1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate question based on context\n",
    "def generate_question(context):\n",
    "    input_text = \"generate question: \" + context\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_ids)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.notebook import tqdm\n",
    "# # Apply the function to generate questions\n",
    "# df['generated_question'] = df['context'].apply(generate_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "# Get total number of rows for progress calculation\n",
    "total_rows = len(df)\n",
    "\n",
    "# Apply the function with a progress bar\n",
    "generated_questions = []\n",
    "for index, row in tqdm(df.iterrows(), total=total_rows, desc=\"Processing rows\", unit=\"row\"):\n",
    "    generated_question = generate_question(row['context'])\n",
    "    generated_questions.append(generated_question)\n",
    "\n",
    "# Assign generated questions back to the DataFrame\n",
    "df['generated_question'] = generated_questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    qa_pairs.append({\n",
    "        \"question\": row['generated_question'],\n",
    "        \"context\": row['context']\n",
    "    })\n",
    "\n",
    "# Example output\n",
    "print(qa_pairs[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_data, val_data = train_test_split(qa_pairs, test_size=0.2)\n",
    "\n",
    "# Format the data for Hugging Face fine-tuning\n",
    "train_df = pd.DataFrame(train_data)\n",
    "val_df = pd.DataFrame(val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForQuestionAnswering, BertTokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load BERT model and tokenizer\n",
    "model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the QA data\n",
    "def tokenize_qa(examples):\n",
    "    inputs = tokenizer(examples['question'], examples['context'], truncation=True, padding=True)\n",
    "    return inputs\n",
    "\n",
    "# Convert train and validation data to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_qa, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_qa, batched=True)\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
