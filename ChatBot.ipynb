{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/stress_urinary_incontinence.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    # Remove special characters, digits, and extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text.lower()  # Convert to lowercase\n",
    "\n",
    "# Apply the cleaning function to the text column\n",
    "df['cleaned_text'] = df['FOI_TEXT'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Function to segment text into sentences\n",
    "def segment_text(text):\n",
    "    return nltk.sent_tokenize(text)\n",
    "\n",
    "# Apply the function to segment the cleaned text\n",
    "df['segmented_text'] = df['cleaned_text'].apply(segment_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load a pre-trained NER model for medical text\n",
    "nlp = spacy.load(\"en_core_sci_md\")  # SciSpacy model\n",
    "\n",
    "# Function to extract entities\n",
    "def extract_context(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([ent.text for ent in doc.ents])  # Join entities as potential context\n",
    "\n",
    "# Apply the function to extract context\n",
    "df['context'] = df['cleaned_text'].apply(extract_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load pre-trained T5 model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"ramsrigouthamg/t5_squad_v1\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"ramsrigouthamg/t5_squad_v1\")\n",
    "\n",
    "# Function to generate question based on context\n",
    "def generate_question(context):\n",
    "    input_text = \"generate question: \" + context\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_ids)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Apply the function to generate questions\n",
    "df['generated_question'] = df['context'].apply(generate_question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    qa_pairs.append({\n",
    "        \"question\": row['generated_question'],\n",
    "        \"context\": row['context']\n",
    "    })\n",
    "\n",
    "# Example output\n",
    "print(qa_pairs[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_data, val_data = train_test_split(qa_pairs, test_size=0.2)\n",
    "\n",
    "# Format the data for Hugging Face fine-tuning\n",
    "train_df = pd.DataFrame(train_data)\n",
    "val_df = pd.DataFrame(val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForQuestionAnswering, BertTokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load BERT model and tokenizer\n",
    "model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the QA data\n",
    "def tokenize_qa(examples):\n",
    "    inputs = tokenizer(examples['question'], examples['context'], truncation=True, padding=True)\n",
    "    return inputs\n",
    "\n",
    "# Convert train and validation data to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_qa, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_qa, batched=True)\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
