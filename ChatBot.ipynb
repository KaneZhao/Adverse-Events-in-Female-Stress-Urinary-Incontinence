{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/stress_urinary_incontinence.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    # Remove special characters, digits, and extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text.lower()  # Convert to lowercase\n",
    "\n",
    "# Apply the cleaning function to the text column\n",
    "df['cleaned_text'] = df['FOI_TEXT'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/zhaohengchuan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to segment text into sentences\n",
    "def segment_text(text):\n",
    "    return nltk.sent_tokenize(text)\n",
    "\n",
    "# Apply the function to segment the cleaned text\n",
    "df['segmented_text'] = df['cleaned_text'].apply(segment_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fba63a4c22044ec9e89ffba059c3e15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a49462396904e90b425c97e94a402d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e1c27cc9614a21b56e20e9a9407be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/31.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f116c0753bf546d282ac5c6f04513fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9664a46823ae41ceb27cd8ffbe20773e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813c3ef9bb5f41b3aaa9d67b31f23d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load T5 model and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('valhalla/t5-small-qg-hl')\n",
    "model = T5ForConditionalGeneration.from_pretrained('valhalla/t5-small-qg-hl')\n",
    "\n",
    "def generate_questions(text):\n",
    "    # Prepare the input for the model\n",
    "    input_text = f\"generate question: {text}\"\n",
    "    inputs = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    \n",
    "    # Generate questions\n",
    "    outputs = model.generate(inputs, max_length=150, num_beams=5, early_stopping=True)\n",
    "    question = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return question\n",
    "\n",
    "# Example of generating questions for each sentence\n",
    "for index, row in df.iterrows():\n",
    "    questions = []\n",
    "    for sentence in row['segmented_text']:\n",
    "        question = generate_questions(sentence)\n",
    "        questions.append(question)\n",
    "    df.at[index, 'questions'] = questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-28 21:23:19.236339: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d4a04189c84815a3d284d9f282d811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/451 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2d25b0d72414e4e9e051608879872c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1891667ba5434298d4e5b61b05cae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4497112ebda14a42b4721551e4637c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef257da61bdd4173a13cb3e08f99de8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            question  \\\n",
      "0                       What is a medtronic product?   \n",
      "1                 What is the name of the complaint?   \n",
      "2  What will a supplemental report be issued if i...   \n",
      "3  What was the manufacturer reference number b4 ...   \n",
      "4  What was the preoperative and postoperative di...   \n",
      "\n",
      "                                              answer  \n",
      "0  BASED ON ADDITIONAL INFORMATION RECEIVED THIS ...  \n",
      "1                                  MEDTRONIC PRODUCT  \n",
      "2               A SUPPLEMENTAL REPORT WILL BE ISSUED  \n",
      "3                                              (B)(4  \n",
      "4                                            EROSION  \n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained QA model\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
    "\n",
    "def generate_answer(question, context):\n",
    "    # Extract the answer from the context based on the question\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    return result['answer']\n",
    "\n",
    "# Generate answers based on the generated questions\n",
    "qa_pairs = []\n",
    "for index, row in df.iterrows():\n",
    "    context = row['FOI_TEXT']\n",
    "    for question in row['questions']:\n",
    "        answer = generate_answer(question, context)\n",
    "        qa_pairs.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "\n",
    "# Convert QA pairs to DataFrame\n",
    "qa_df = pd.DataFrame(qa_pairs)\n",
    "print(qa_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load your QA pairs\n",
    "# qa_df = pd.read_csv('generated_qa_pairs.csv')\n",
    "\n",
    "# Prepare dataset for fine-tuning\n",
    "data = []\n",
    "for index, row in qa_df.iterrows():\n",
    "    data.append({\"input\": row['question'], \"output\": row['answer']})\n",
    "\n",
    "# Create a Hugging Face Dataset\n",
    "dataset = Dataset.from_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a198c8be8d864ff8844ae2b994066d32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the input and output\n",
    "    input_ids = tokenizer(examples['input'], truncation=True, padding='max_length', max_length=50)\n",
    "    output_ids = tokenizer(examples['output'], truncation=True, padding='max_length', max_length=50)\n",
    "    \n",
    "    # Prepare labels (shifted input)\n",
    "    labels = output_ids['input_ids']\n",
    "    \n",
    "    # Return the tokenized inputs and labels\n",
    "    return {\n",
    "        'input_ids': input_ids['input_ids'],\n",
    "        'attention_mask': input_ids['attention_mask'],\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b18dbbe1664978b434eee67edfad1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 44.8309, 'train_samples_per_second': 0.669, 'train_steps_per_second': 0.335, 'train_loss': 3.2952044169108072, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_gpt2/tokenizer_config.json',\n",
       " './fine_tuned_gpt2/special_tokens_map.json',\n",
       " './fine_tuned_gpt2/vocab.json',\n",
       " './fine_tuned_gpt2/merges.txt',\n",
       " './fine_tuned_gpt2/added_tokens.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('./fine_tuned_gpt2')\n",
    "tokenizer.save_pretrained('./fine_tuned_gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_answer(question, max_length=50):\n",
    "    # Tokenize the input question\n",
    "    inputs = tokenizer.encode(question, return_tensors='pt')  # Add batch dimension\n",
    "\n",
    "    # Generate the response\n",
    "    with torch.no_grad():  # Disable gradient calculations\n",
    "        outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1)\n",
    "\n",
    "    # Decode the generated tokens\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is stress urinary incontinence?\n",
      "Answer: What is stress urinary incontinence?\n"
     ]
    }
   ],
   "source": [
    "# Example question\n",
    "question = \"What is stress urinary incontinence?\"\n",
    "\n",
    "# Generate an answer\n",
    "answer = generate_answer(question)\n",
    "\n",
    "# Print the answer\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/spacy/language.py:2195: FutureWarning: Possible set union at position 6328\n",
      "  deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load a pre-trained NER model for medical text\n",
    "nlp = spacy.load(\"en_core_sci_md\")  # SciSpacy model\n",
    "\n",
    "# Function to extract entities\n",
    "def extract_context(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([ent.text for ent in doc.ents])  # Join entities as potential context\n",
    "\n",
    "# Apply the function to extract context\n",
    "df['context'] = df['cleaned_text'].apply(extract_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load pre-trained T5 model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"ramsrigouthamg/t5_squad_v1\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"ramsrigouthamg/t5_squad_v1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate question based on context\n",
    "def generate_question(context):\n",
    "    input_text = \"generate question: \" + context\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_ids)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.notebook import tqdm\n",
    "# # Apply the function to generate questions\n",
    "# df['generated_question'] = df['context'].apply(generate_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "# Get total number of rows for progress calculation\n",
    "total_rows = len(df)\n",
    "\n",
    "# Apply the function with a progress bar\n",
    "generated_questions = []\n",
    "for index, row in tqdm(df.iterrows(), total=total_rows, desc=\"Processing rows\", unit=\"row\"):\n",
    "    generated_question = generate_question(row['context'])\n",
    "    generated_questions.append(generated_question)\n",
    "\n",
    "# Assign generated questions back to the DataFrame\n",
    "df['generated_question'] = generated_questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    qa_pairs.append({\n",
    "        \"question\": row['generated_question'],\n",
    "        \"context\": row['context']\n",
    "    })\n",
    "\n",
    "# Example output\n",
    "print(qa_pairs[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_data, val_data = train_test_split(qa_pairs, test_size=0.2)\n",
    "\n",
    "# Format the data for Hugging Face fine-tuning\n",
    "train_df = pd.DataFrame(train_data)\n",
    "val_df = pd.DataFrame(val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForQuestionAnswering, BertTokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load BERT model and tokenizer\n",
    "model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the QA data\n",
    "def tokenize_qa(examples):\n",
    "    inputs = tokenizer(examples['question'], examples['context'], truncation=True, padding=True)\n",
    "    return inputs\n",
    "\n",
    "# Convert train and validation data to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_qa, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_qa, batched=True)\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
